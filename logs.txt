
==> Audit <==
|---------|------------------------------------------------------------------------------------------------------|----------|--------|---------|----------------------|----------------------|
| Command |                                                 Args                                                 | Profile  |  User  | Version |      Start Time      |       End Time       |
|---------|------------------------------------------------------------------------------------------------------|----------|--------|---------|----------------------|----------------------|
| start   |                                                                                                      | minikube | wweber | v1.34.0 | 27 Dec 24 17:36 SAST | 27 Dec 24 18:49 SAST |
| ssh     | sudo mkdir -p                                                                                        | minikube | wweber | v1.34.0 | 01 Jan 25 03:58 SAST | 01 Jan 25 03:58 SAST |
|         | /usr/local/share/ca-certificates                                                                     |          |        |         |                      |                      |
| cp      | ./reactory-express-server/certificates/ca-certificates.crt                                           | minikube | wweber | v1.34.0 | 01 Jan 25 04:00 SAST |                      |
|         | /usr/local/share/ca-certificates                                                                     |          |        |         |                      |                      |
| cp      | ./reactory-express-server/certificates/ca-certificates.crt                                           | minikube | wweber | v1.34.0 | 01 Jan 25 04:00 SAST | 01 Jan 25 04:00 SAST |
|         | /usr/local/share/ca-certificates/cato.crt                                                            |          |        |         |                      |                      |
| ssh     | sudo update-ca-certificates                                                                          | minikube | wweber | v1.34.0 | 01 Jan 25 04:01 SAST |                      |
| stop    |                                                                                                      | minikube | wweber | v1.34.0 | 01 Jan 25 04:01 SAST | 01 Jan 25 04:02 SAST |
| start   |                                                                                                      | minikube | wweber | v1.34.0 | 01 Jan 25 04:04 SAST | 01 Jan 25 04:06 SAST |
| addons  | enable ingress                                                                                       | minikube | wweber | v1.34.0 | 01 Jan 25 13:07 SAST |                      |
| addons  | enable ingress                                                                                       | minikube | wweber | v1.34.0 | 02 Jan 25 02:39 SAST |                      |
| ssh     | -- sudo mkdir -p                                                                                     | minikube | wweber | v1.34.0 | 02 Jan 25 02:55 SAST | 02 Jan 25 02:55 SAST |
|         | /etc/docker/certs.d                                                                                  |          |        |         |                      |                      |
| cp      | ./certificates/ca-certificates.crt                                                                   | minikube | wweber | v1.34.0 | 02 Jan 25 02:56 SAST | 02 Jan 25 02:56 SAST |
|         | /etc/docker/certs.d/cato.crt                                                                         |          |        |         |                      |                      |
| ssh     | systemctl restart docker                                                                             | minikube | wweber | v1.34.0 | 02 Jan 25 02:57 SAST |                      |
| ssh     | -- sudo systemctl restart                                                                            | minikube | wweber | v1.34.0 | 02 Jan 25 02:57 SAST | 02 Jan 25 02:57 SAST |
|         | docker                                                                                               |          |        |         |                      |                      |
| addons  | enable ingress                                                                                       | minikube | wweber | v1.34.0 | 02 Jan 25 02:58 SAST |                      |
| ssh     |                                                                                                      | minikube | wweber | v1.34.0 | 02 Jan 25 03:15 SAST |                      |
| ssh     |                                                                                                      | minikube | wweber | v1.34.0 | 02 Jan 25 03:19 SAST | 02 Jan 25 03:27 SAST |
| stop    |                                                                                                      | minikube | wweber | v1.34.0 | 02 Jan 25 03:27 SAST | 02 Jan 25 03:29 SAST |
| start   |                                                                                                      | minikube | wweber | v1.34.0 | 02 Jan 25 03:29 SAST | 02 Jan 25 03:30 SAST |
| addons  | enable ingres                                                                                        | minikube | wweber | v1.34.0 | 02 Jan 25 03:31 SAST |                      |
| addons  | enable ingress                                                                                       | minikube | wweber | v1.34.0 | 02 Jan 25 03:31 SAST |                      |
| ssh     |                                                                                                      | minikube | wweber | v1.34.0 | 02 Jan 25 03:39 SAST |                      |
| ssh     |                                                                                                      | minikube | wweber | v1.34.0 | 02 Jan 25 03:47 SAST |                      |
| image   | load                                                                                                 | minikube | wweber | v1.34.0 | 09 Jan 25 03:44 SAST |                      |
|         | localhost/reactory/reactory-express-server:1.0.0                                                     |          |        |         |                      |                      |
| image   | load                                                                                                 | minikube | wweber | v1.34.0 | 09 Jan 25 03:51 SAST | 09 Jan 25 03:53 SAST |
|         | ./build/reactory-express-server.tar                                                                  |          |        |         |                      |                      |
| start   |                                                                                                      | minikube | wweber | v1.34.0 | 10 Jan 25 03:34 SAST | 10 Jan 25 03:35 SAST |
| stop    |                                                                                                      | minikube | wweber | v1.34.0 | 10 Jan 25 04:50 SAST | 10 Jan 25 04:52 SAST |
| start   |                                                                                                      | minikube | wweber | v1.34.0 | 10 Jan 25 04:52 SAST | 10 Jan 25 04:52 SAST |
| ssh     |                                                                                                      | minikube | wweber | v1.34.0 | 10 Jan 25 16:45 SAST | 10 Jan 25 16:50 SAST |
| mount   | /Users/wweber/Source/reactory:/etc/reactory                                                          | minikube | wweber | v1.34.0 | 10 Jan 25 17:31 SAST |                      |
| ssh     |                                                                                                      | minikube | wweber | v1.34.0 | 10 Jan 25 17:39 SAST | 10 Jan 25 17:40 SAST |
| cp      | ./reactory-express-server/config/reactory/.env.container                                             | minikube | wweber | v1.34.0 | 10 Jan 25 17:41 SAST | 10 Jan 25 17:41 SAST |
|         | /Users/wweber/reactory/.env                                                                          |          |        |         |                      |                      |
| ssh     |                                                                                                      | minikube | wweber | v1.34.0 | 10 Jan 25 17:41 SAST | 10 Jan 25 17:42 SAST |
| cp      | ./config/reactory/.env.local                                                                         | minikube | wweber | v1.34.0 | 11 Jan 25 02:20 SAST | 11 Jan 25 02:20 SAST |
|         | /etc/reactory/.env                                                                                   |          |        |         |                      |                      |
| cp      | /Users/wweber/Source/reactory/reactory-express-server/src/modules/reactory-telemetry/data/grafana    | minikube | wweber | v1.34.0 | 11 Jan 25 02:20 SAST |                      |
|         | /etc/grafana                                                                                         |          |        |         |                      |                      |
| cp      | /Users/wweber/Source/reactory/reactory-express-server/src/modules/reactory-telemetry/data/prometheus | minikube | wweber | v1.34.0 | 11 Jan 25 02:20 SAST |                      |
|         | /etc/prometheus                                                                                      |          |        |         |                      |                      |
| cp      | reactory/build/data                                                                                  | minikube | wweber | v1.34.0 | 11 Jan 25 02:20 SAST |                      |
|         | /var/reactory/reactory-data                                                                          |          |        |         |                      |                      |
|---------|------------------------------------------------------------------------------------------------------|----------|--------|---------|----------------------|----------------------|


==> Last Start <==
Log file created at: 2025/01/10 04:52:09
Running on machine: Zepz-HPX62VLX2V
Binary: Built with gc go1.23.1 for darwin/arm64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0110 04:52:09.464828    5632 out.go:345] Setting OutFile to fd 1 ...
I0110 04:52:09.465398    5632 out.go:397] isatty.IsTerminal(1) = true
I0110 04:52:09.465401    5632 out.go:358] Setting ErrFile to fd 2...
I0110 04:52:09.465403    5632 out.go:397] isatty.IsTerminal(2) = true
I0110 04:52:09.465526    5632 root.go:338] Updating PATH: /Users/wweber/.minikube/bin
W0110 04:52:09.465590    5632 root.go:314] Error reading config file at /Users/wweber/.minikube/config/config.json: open /Users/wweber/.minikube/config/config.json: no such file or directory
I0110 04:52:09.467339    5632 out.go:352] Setting JSON to false
I0110 04:52:09.498968    5632 start.go:129] hostinfo: {"hostname":"Zepz-HPX62VLX2V","uptime":1961187,"bootTime":1734516342,"procs":533,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"15.2","kernelVersion":"24.2.0","kernelArch":"arm64","virtualizationSystem":"","virtualizationRole":"","hostId":"9a8c69ba-dfa2-5c03-9d87-d7e37dc2b05b"}
W0110 04:52:09.499120    5632 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0110 04:52:09.504723    5632 out.go:177] 😄  minikube v1.34.0 on Darwin 15.2 (arm64)
I0110 04:52:09.512827    5632 notify.go:220] Checking for updates...
I0110 04:52:09.513248    5632 config.go:182] Loaded profile config "minikube": Driver=parallels, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I0110 04:52:09.513398    5632 driver.go:394] Setting default libvirt URI to qemu:///system
I0110 04:52:09.518749    5632 out.go:177] ✨  Using the parallels driver based on existing profile
I0110 04:52:09.526816    5632 start.go:297] selected driver: parallels
I0110 04:52:09.526819    5632 start.go:901] validating driver "parallels" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.34.0-arm64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:6000 CPUs:2 DiskSize:20000 Driver:parallels HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:10.211.55.4 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0110 04:52:09.526884    5632 start.go:912] status for parallels: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0110 04:52:09.527045    5632 cni.go:84] Creating CNI manager for ""
I0110 04:52:09.527142    5632 cni.go:158] "parallels" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0110 04:52:09.527179    5632 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.34.0-arm64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:6000 CPUs:2 DiskSize:20000 Driver:parallels HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:10.211.55.4 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0110 04:52:09.527256    5632 iso.go:125] acquiring lock: {Name:mk3581e2f9b873212787d7e893d2c89d28d2c86c Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0110 04:52:09.535700    5632 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I0110 04:52:09.539639    5632 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I0110 04:52:09.539652    5632 preload.go:146] Found local preload: /Users/wweber/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.31.0-docker-overlay2-arm64.tar.lz4
I0110 04:52:09.539683    5632 cache.go:56] Caching tarball of preloaded images
I0110 04:52:09.539880    5632 preload.go:172] Found /Users/wweber/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.31.0-docker-overlay2-arm64.tar.lz4 in cache, skipping download
I0110 04:52:09.539894    5632 cache.go:59] Finished verifying existence of preloaded tar for v1.31.0 on docker
I0110 04:52:09.539946    5632 profile.go:143] Saving config to /Users/wweber/.minikube/profiles/minikube/config.json ...
I0110 04:52:09.540620    5632 start.go:360] acquireMachinesLock for minikube: {Name:mkeee5f7130a97b59c00e2316a2f37ccff868ae8 Clock:{} Delay:500ms Timeout:13m0s Cancel:<nil>}
I0110 04:52:09.540722    5632 start.go:364] duration metric: took 97.541µs to acquireMachinesLock for "minikube"
I0110 04:52:09.540729    5632 start.go:96] Skipping create...Using existing machine configuration
I0110 04:52:09.540731    5632 fix.go:54] fixHost starting: 
I0110 04:52:09.540872    5632 main.go:141] libmachine: executing: /usr/local/bin/prlctl list minikube --output status --no-header
I0110 04:52:09.736079    5632 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0110 04:52:09.736123    5632 fix.go:138] unexpected machine state, will restart: <nil>
I0110 04:52:09.741293    5632 out.go:177] 🔄  Restarting existing parallels VM for "minikube" ...
I0110 04:52:09.749245    5632 main.go:141] libmachine: executing: /usr/local/bin/prlsrvctl net info Shared
I0110 04:52:10.003068    5632 main.go:141] libmachine: IP address of Shared network adapter: 10.211.55.2
I0110 04:52:10.003236    5632 main.go:141] libmachine: All host interface addressess: [127.0.0.1/8 ::1/128 fe80::1/64 fe80::189c:c713:e963:4999/64 192.168.0.8/24 fe80::ac27:ecff:fe51:7006/64 fe80::ac27:ecff:fe51:7006/64 fe80::d954:8d6e:7206:e811/64 fe80::6616:be6e:d31e:4d47/64 fe80::fde8:5b84:7cc0:45c/64 fe80::ce81:b1c:bd2c:69e/64 10.1.242.5/32 10.211.55.2/24 fe80::5ce9:1eff:fe97:3a64/64 fdb2:2c26:f4e4::1/64 10.37.129.2/24 fe80::5ce9:1eff:fe97:3a65/64 fdb2:2c26:f4e4:1::1/64]
I0110 04:52:10.003244    5632 main.go:141] libmachine: Parallels Shared network adapter is connected
I0110 04:52:10.003259    5632 main.go:141] libmachine: executing: /usr/local/bin/prlctl list minikube --output status --no-header
I0110 04:52:10.202307    5632 main.go:141] libmachine: executing: /usr/local/bin/prlctl start minikube
I0110 04:52:10.765718    5632 main.go:141] libmachine: Waiting for VM to start...
I0110 04:52:10.765755    5632 main.go:141] libmachine: Getting to WaitForSSH function...
I0110 04:52:10.765778    5632 main.go:141] libmachine: executing: /usr/local/bin/prlctl list minikube --output status --no-header
I0110 04:52:10.923168    5632 main.go:141] libmachine: executing: /usr/local/bin/prlctl list -i minikube
I0110 04:52:12.042596    5632 main.go:141] libmachine: Found lease: 10.211.55.4 for MAC: 001C42B2DA18, expiring at 1736479182, leased for 1800 s.

I0110 04:52:12.042616    5632 main.go:141] libmachine: Found IP lease: 10.211.55.4 for MAC address 001C42B2DA18

I0110 04:52:12.042705    5632 main.go:141] libmachine: Using SSH client type: native
I0110 04:52:12.043208    5632 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x102fbd4b0] 0x102fbfcf0 <nil>  [] 0s} 10.211.55.4 22 <nil> <nil>}
I0110 04:52:12.043211    5632 main.go:141] libmachine: About to run SSH command:
exit 0
I0110 04:52:19.839316    5632 main.go:141] libmachine: Error dialing TCP: dial tcp 10.211.55.4:22: connect: operation timed out
I0110 04:52:22.872013    5632 main.go:141] libmachine: Error dialing TCP: dial tcp 10.211.55.4:22: connect: connection refused
I0110 04:52:25.927774    5632 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0110 04:52:25.927787    5632 main.go:141] libmachine: executing: /usr/local/bin/prlctl list minikube --output status --no-header
I0110 04:52:26.128156    5632 main.go:141] libmachine: executing: /usr/local/bin/prlctl list -i minikube
I0110 04:52:26.351159    5632 main.go:141] libmachine: Found lease: 10.211.55.4 for MAC: 001C42B2DA18, expiring at 1736479340, leased for 1800 s.

I0110 04:52:26.351181    5632 main.go:141] libmachine: Found IP lease: 10.211.55.4 for MAC address 001C42B2DA18

I0110 04:52:26.351198    5632 main.go:141] libmachine: executing: /usr/local/bin/prlctl list --info minikube
I0110 04:52:26.574469    5632 main.go:141] libmachine: executing: /usr/local/bin/prlctl list minikube --output status --no-header
I0110 04:52:27.643563    5632 main.go:141] libmachine: executing: /usr/local/bin/prlctl list -i minikube
I0110 04:52:27.872111    5632 main.go:141] libmachine: Found lease: 10.211.55.4 for MAC: 001C42B2DA18, expiring at 1736479340, leased for 1800 s.

I0110 04:52:27.872128    5632 main.go:141] libmachine: Found IP lease: 10.211.55.4 for MAC address 001C42B2DA18

I0110 04:52:27.872192    5632 profile.go:143] Saving config to /Users/wweber/.minikube/profiles/minikube/config.json ...
I0110 04:52:27.872870    5632 machine.go:93] provisionDockerMachine start ...
I0110 04:52:27.872889    5632 main.go:141] libmachine: executing: /usr/local/bin/prlctl list minikube --output status --no-header
I0110 04:52:28.054686    5632 main.go:141] libmachine: executing: /usr/local/bin/prlctl list -i minikube
I0110 04:52:28.251830    5632 main.go:141] libmachine: Found lease: 10.211.55.4 for MAC: 001C42B2DA18, expiring at 1736479340, leased for 1800 s.

I0110 04:52:28.251850    5632 main.go:141] libmachine: Found IP lease: 10.211.55.4 for MAC address 001C42B2DA18

I0110 04:52:28.251954    5632 main.go:141] libmachine: Using SSH client type: native
I0110 04:52:28.252119    5632 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x102fbd4b0] 0x102fbfcf0 <nil>  [] 0s} 10.211.55.4 22 <nil> <nil>}
I0110 04:52:28.252121    5632 main.go:141] libmachine: About to run SSH command:
hostname
I0110 04:52:28.294446    5632 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0110 04:52:28.294479    5632 buildroot.go:166] provisioning hostname "minikube"
I0110 04:52:28.294502    5632 main.go:141] libmachine: executing: /usr/local/bin/prlctl list minikube --output status --no-header
I0110 04:52:28.486920    5632 main.go:141] libmachine: executing: /usr/local/bin/prlctl list -i minikube
I0110 04:52:28.691829    5632 main.go:141] libmachine: Found lease: 10.211.55.4 for MAC: 001C42B2DA18, expiring at 1736479340, leased for 1800 s.

I0110 04:52:28.691852    5632 main.go:141] libmachine: Found IP lease: 10.211.55.4 for MAC address 001C42B2DA18

I0110 04:52:28.691953    5632 main.go:141] libmachine: Using SSH client type: native
I0110 04:52:28.692103    5632 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x102fbd4b0] 0x102fbfcf0 <nil>  [] 0s} 10.211.55.4 22 <nil> <nil>}
I0110 04:52:28.692106    5632 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0110 04:52:28.742685    5632 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0110 04:52:28.742698    5632 main.go:141] libmachine: executing: /usr/local/bin/prlctl list minikube --output status --no-header
I0110 04:52:28.924979    5632 main.go:141] libmachine: executing: /usr/local/bin/prlctl list -i minikube
I0110 04:52:29.134715    5632 main.go:141] libmachine: Found lease: 10.211.55.4 for MAC: 001C42B2DA18, expiring at 1736479340, leased for 1800 s.

I0110 04:52:29.134733    5632 main.go:141] libmachine: Found IP lease: 10.211.55.4 for MAC address 001C42B2DA18

I0110 04:52:29.134831    5632 main.go:141] libmachine: Using SSH client type: native
I0110 04:52:29.134981    5632 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x102fbd4b0] 0x102fbfcf0 <nil>  [] 0s} 10.211.55.4 22 <nil> <nil>}
I0110 04:52:29.134987    5632 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0110 04:52:29.181013    5632 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0110 04:52:29.181027    5632 buildroot.go:172] set auth options {CertDir:/Users/wweber/.minikube CaCertPath:/Users/wweber/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/wweber/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/wweber/.minikube/machines/server.pem ServerKeyPath:/Users/wweber/.minikube/machines/server-key.pem ClientKeyPath:/Users/wweber/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/wweber/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/wweber/.minikube}
I0110 04:52:29.181043    5632 buildroot.go:174] setting up certificates
I0110 04:52:29.181048    5632 provision.go:84] configureAuth start
I0110 04:52:29.181058    5632 main.go:141] libmachine: executing: /usr/local/bin/prlctl list minikube --output status --no-header
I0110 04:52:29.393023    5632 main.go:141] libmachine: executing: /usr/local/bin/prlctl list -i minikube
I0110 04:52:29.607014    5632 main.go:141] libmachine: Found lease: 10.211.55.4 for MAC: 001C42B2DA18, expiring at 1736479340, leased for 1800 s.

I0110 04:52:29.607031    5632 main.go:141] libmachine: Found IP lease: 10.211.55.4 for MAC address 001C42B2DA18

I0110 04:52:29.607041    5632 main.go:141] libmachine: executing: /usr/local/bin/prlctl list minikube --output status --no-header
I0110 04:52:29.800458    5632 main.go:141] libmachine: executing: /usr/local/bin/prlctl list -i minikube
I0110 04:52:30.022646    5632 main.go:141] libmachine: Found lease: 10.211.55.4 for MAC: 001C42B2DA18, expiring at 1736479340, leased for 1800 s.

I0110 04:52:30.022670    5632 main.go:141] libmachine: Found IP lease: 10.211.55.4 for MAC address 001C42B2DA18

I0110 04:52:30.022673    5632 provision.go:143] copyHostCerts
I0110 04:52:30.022774    5632 exec_runner.go:144] found /Users/wweber/.minikube/ca.pem, removing ...
I0110 04:52:30.022779    5632 exec_runner.go:203] rm: /Users/wweber/.minikube/ca.pem
I0110 04:52:30.023056    5632 exec_runner.go:151] cp: /Users/wweber/.minikube/certs/ca.pem --> /Users/wweber/.minikube/ca.pem (1078 bytes)
I0110 04:52:30.024406    5632 exec_runner.go:144] found /Users/wweber/.minikube/cert.pem, removing ...
I0110 04:52:30.024409    5632 exec_runner.go:203] rm: /Users/wweber/.minikube/cert.pem
I0110 04:52:30.024613    5632 exec_runner.go:151] cp: /Users/wweber/.minikube/certs/cert.pem --> /Users/wweber/.minikube/cert.pem (1123 bytes)
I0110 04:52:30.025035    5632 exec_runner.go:144] found /Users/wweber/.minikube/key.pem, removing ...
I0110 04:52:30.025037    5632 exec_runner.go:203] rm: /Users/wweber/.minikube/key.pem
I0110 04:52:30.025240    5632 exec_runner.go:151] cp: /Users/wweber/.minikube/certs/key.pem --> /Users/wweber/.minikube/key.pem (1675 bytes)
I0110 04:52:30.025594    5632 provision.go:117] generating server cert: /Users/wweber/.minikube/machines/server.pem ca-key=/Users/wweber/.minikube/certs/ca.pem private-key=/Users/wweber/.minikube/certs/ca-key.pem org=wweber.minikube san=[10.211.55.4 127.0.0.1 localhost minikube]
I0110 04:52:30.138889    5632 provision.go:177] copyRemoteCerts
I0110 04:52:30.139210    5632 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0110 04:52:30.139221    5632 main.go:141] libmachine: executing: /usr/local/bin/prlctl list minikube --output status --no-header
I0110 04:52:30.321183    5632 main.go:141] libmachine: executing: /usr/local/bin/prlctl list -i minikube
I0110 04:52:30.522414    5632 main.go:141] libmachine: Found lease: 10.211.55.4 for MAC: 001C42B2DA18, expiring at 1736479340, leased for 1800 s.

I0110 04:52:30.522429    5632 main.go:141] libmachine: Found IP lease: 10.211.55.4 for MAC address 001C42B2DA18

I0110 04:52:30.522436    5632 sshutil.go:53] new ssh client: &{IP:10.211.55.4 Port:22 SSHKeyPath:/Users/wweber/.minikube/machines/minikube/id_rsa Username:docker}
I0110 04:52:30.548226    5632 ssh_runner.go:362] scp /Users/wweber/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1078 bytes)
I0110 04:52:30.558835    5632 ssh_runner.go:362] scp /Users/wweber/.minikube/machines/server.pem --> /etc/docker/server.pem (1180 bytes)
I0110 04:52:30.568937    5632 ssh_runner.go:362] scp /Users/wweber/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0110 04:52:30.579185    5632 provision.go:87] duration metric: took 1.398142458s to configureAuth
I0110 04:52:30.579191    5632 buildroot.go:189] setting minikube options for container-runtime
I0110 04:52:30.579374    5632 config.go:182] Loaded profile config "minikube": Driver=parallels, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I0110 04:52:30.579390    5632 main.go:141] libmachine: executing: /usr/local/bin/prlctl list minikube --output status --no-header
I0110 04:52:31.603392    5632 main.go:141] libmachine: executing: /usr/local/bin/prlctl list -i minikube
I0110 04:52:31.860260    5632 main.go:141] libmachine: Found lease: 10.211.55.4 for MAC: 001C42B2DA18, expiring at 1736479340, leased for 1800 s.

I0110 04:52:31.860280    5632 main.go:141] libmachine: Found IP lease: 10.211.55.4 for MAC address 001C42B2DA18

I0110 04:52:31.860382    5632 main.go:141] libmachine: Using SSH client type: native
I0110 04:52:31.860530    5632 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x102fbd4b0] 0x102fbfcf0 <nil>  [] 0s} 10.211.55.4 22 <nil> <nil>}
I0110 04:52:31.860534    5632 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0110 04:52:31.905586    5632 main.go:141] libmachine: SSH cmd err, output: <nil>: tmpfs

I0110 04:52:31.905595    5632 buildroot.go:70] root file system type: tmpfs
I0110 04:52:31.905675    5632 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0110 04:52:31.905697    5632 main.go:141] libmachine: executing: /usr/local/bin/prlctl list minikube --output status --no-header
I0110 04:52:32.116231    5632 main.go:141] libmachine: executing: /usr/local/bin/prlctl list -i minikube
I0110 04:52:32.342261    5632 main.go:141] libmachine: Found lease: 10.211.55.4 for MAC: 001C42B2DA18, expiring at 1736479340, leased for 1800 s.

I0110 04:52:32.342284    5632 main.go:141] libmachine: Found IP lease: 10.211.55.4 for MAC address 001C42B2DA18

I0110 04:52:32.342395    5632 main.go:141] libmachine: Using SSH client type: native
I0110 04:52:32.342526    5632 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x102fbd4b0] 0x102fbfcf0 <nil>  [] 0s} 10.211.55.4 22 <nil> <nil>}
I0110 04:52:32.342557    5632 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target  minikube-automount.service docker.socket
Requires= minikube-automount.service docker.socket 
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=parallels --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0110 04:52:32.393201    5632 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target  minikube-automount.service docker.socket
Requires= minikube-automount.service docker.socket 
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=parallels --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0110 04:52:32.393220    5632 main.go:141] libmachine: executing: /usr/local/bin/prlctl list minikube --output status --no-header
I0110 04:52:32.569222    5632 main.go:141] libmachine: executing: /usr/local/bin/prlctl list -i minikube
I0110 04:52:32.770789    5632 main.go:141] libmachine: Found lease: 10.211.55.4 for MAC: 001C42B2DA18, expiring at 1736479340, leased for 1800 s.

I0110 04:52:32.770807    5632 main.go:141] libmachine: Found IP lease: 10.211.55.4 for MAC address 001C42B2DA18

I0110 04:52:32.770894    5632 main.go:141] libmachine: Using SSH client type: native
I0110 04:52:32.771033    5632 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x102fbd4b0] 0x102fbfcf0 <nil>  [] 0s} 10.211.55.4 22 <nil> <nil>}
I0110 04:52:32.771041    5632 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0110 04:52:34.436862    5632 main.go:141] libmachine: SSH cmd err, output: <nil>: diff: can't stat '/lib/systemd/system/docker.service': No such file or directory
Created symlink /etc/systemd/system/multi-user.target.wants/docker.service → /usr/lib/systemd/system/docker.service.

I0110 04:52:34.436888    5632 machine.go:96] duration metric: took 6.564135708s to provisionDockerMachine
I0110 04:52:34.436915    5632 start.go:293] postStartSetup for "minikube" (driver="parallels")
I0110 04:52:34.436934    5632 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0110 04:52:34.437372    5632 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0110 04:52:34.437411    5632 main.go:141] libmachine: executing: /usr/local/bin/prlctl list minikube --output status --no-header
I0110 04:52:34.631460    5632 main.go:141] libmachine: executing: /usr/local/bin/prlctl list -i minikube
I0110 04:52:34.854232    5632 main.go:141] libmachine: Found lease: 10.211.55.4 for MAC: 001C42B2DA18, expiring at 1736479340, leased for 1800 s.

I0110 04:52:34.854254    5632 main.go:141] libmachine: Found IP lease: 10.211.55.4 for MAC address 001C42B2DA18

I0110 04:52:34.854260    5632 sshutil.go:53] new ssh client: &{IP:10.211.55.4 Port:22 SSHKeyPath:/Users/wweber/.minikube/machines/minikube/id_rsa Username:docker}
I0110 04:52:34.879800    5632 ssh_runner.go:195] Run: cat /etc/os-release
I0110 04:52:34.881225    5632 info.go:137] Remote host: Buildroot 2023.02.9
I0110 04:52:34.881241    5632 filesync.go:126] Scanning /Users/wweber/.minikube/addons for local assets ...
I0110 04:52:34.881428    5632 filesync.go:126] Scanning /Users/wweber/.minikube/files for local assets ...
I0110 04:52:34.881571    5632 start.go:296] duration metric: took 444.657541ms for postStartSetup
I0110 04:52:34.881588    5632 fix.go:56] duration metric: took 25.341347917s for fixHost
I0110 04:52:34.881609    5632 main.go:141] libmachine: executing: /usr/local/bin/prlctl list minikube --output status --no-header
I0110 04:52:35.080699    5632 main.go:141] libmachine: executing: /usr/local/bin/prlctl list -i minikube
I0110 04:52:35.300798    5632 main.go:141] libmachine: Found lease: 10.211.55.4 for MAC: 001C42B2DA18, expiring at 1736479340, leased for 1800 s.

I0110 04:52:35.300816    5632 main.go:141] libmachine: Found IP lease: 10.211.55.4 for MAC address 001C42B2DA18

I0110 04:52:35.300912    5632 main.go:141] libmachine: Using SSH client type: native
I0110 04:52:35.301086    5632 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x102fbd4b0] 0x102fbfcf0 <nil>  [] 0s} 10.211.55.4 22 <nil> <nil>}
I0110 04:52:35.301089    5632 main.go:141] libmachine: About to run SSH command:
date +%s.%N
I0110 04:52:35.348882    5632 main.go:141] libmachine: SSH cmd err, output: <nil>: 1736477555.805412966

I0110 04:52:35.348888    5632 fix.go:216] guest clock: 1736477555.805412966
I0110 04:52:35.348892    5632 fix.go:229] Guest: 2025-01-10 04:52:35.805412966 +0200 SAST Remote: 2025-01-10 04:52:34.881589 +0200 SAST m=+25.436654751 (delta=923.823966ms)
I0110 04:52:35.348913    5632 fix.go:200] guest clock delta is within tolerance: 923.823966ms
I0110 04:52:35.348916    5632 start.go:83] releasing machines lock for "minikube", held for 25.808690625s
I0110 04:52:35.348956    5632 main.go:141] libmachine: executing: /usr/local/bin/prlctl list minikube --output status --no-header
I0110 04:52:35.548060    5632 main.go:141] libmachine: executing: /usr/local/bin/prlctl list -i minikube
I0110 04:52:35.769238    5632 main.go:141] libmachine: Found lease: 10.211.55.4 for MAC: 001C42B2DA18, expiring at 1736479340, leased for 1800 s.

I0110 04:52:35.769263    5632 main.go:141] libmachine: Found IP lease: 10.211.55.4 for MAC address 001C42B2DA18

I0110 04:52:35.770048    5632 ssh_runner.go:195] Run: cat /version.json
I0110 04:52:35.770060    5632 main.go:141] libmachine: executing: /usr/local/bin/prlctl list minikube --output status --no-header
I0110 04:52:35.770463    5632 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0110 04:52:35.770705    5632 main.go:141] libmachine: executing: /usr/local/bin/prlctl list minikube --output status --no-header
I0110 04:52:35.922786    5632 main.go:141] libmachine: executing: /usr/local/bin/prlctl list -i minikube
I0110 04:52:35.928685    5632 main.go:141] libmachine: executing: /usr/local/bin/prlctl list -i minikube
I0110 04:52:36.133198    5632 main.go:141] libmachine: Found lease: 10.211.55.4 for MAC: 001C42B2DA18, expiring at 1736479340, leased for 1800 s.

I0110 04:52:36.133217    5632 main.go:141] libmachine: Found IP lease: 10.211.55.4 for MAC address 001C42B2DA18

I0110 04:52:36.133223    5632 sshutil.go:53] new ssh client: &{IP:10.211.55.4 Port:22 SSHKeyPath:/Users/wweber/.minikube/machines/minikube/id_rsa Username:docker}
I0110 04:52:36.133909    5632 main.go:141] libmachine: Found lease: 10.211.55.4 for MAC: 001C42B2DA18, expiring at 1736479340, leased for 1800 s.

I0110 04:52:36.133919    5632 main.go:141] libmachine: Found IP lease: 10.211.55.4 for MAC address 001C42B2DA18

I0110 04:52:36.133922    5632 sshutil.go:53] new ssh client: &{IP:10.211.55.4 Port:22 SSHKeyPath:/Users/wweber/.minikube/machines/minikube/id_rsa Username:docker}
I0110 04:52:36.155439    5632 ssh_runner.go:195] Run: systemctl --version
W0110 04:52:36.248782    5632 start.go:867] [curl -sS -m 2 https://registry.k8s.io/] failed: curl -sS -m 2 https://registry.k8s.io/: Process exited with status 60
stdout:

stderr:
curl: (60) SSL certificate problem: self signed certificate in certificate chain
More details here: https://curl.se/docs/sslcerts.html

curl failed to verify the legitimacy of the server and therefore could not
establish a secure connection to it. To learn more about this situation and
how to fix it, please visit the web page mentioned above.
I0110 04:52:36.248973    5632 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
W0110 04:52:36.257317    5632 cni.go:209] loopback cni configuration skipped: "/etc/cni/net.d/*loopback.conf*" not found
I0110 04:52:36.257424    5632 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0110 04:52:36.270374    5632 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0110 04:52:36.270386    5632 start.go:495] detecting cgroup driver to use...
I0110 04:52:36.270526    5632 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0110 04:52:36.281435    5632 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0110 04:52:36.285465    5632 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0110 04:52:36.289110    5632 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0110 04:52:36.289218    5632 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0110 04:52:36.292709    5632 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0110 04:52:36.295996    5632 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0110 04:52:36.299845    5632 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0110 04:52:36.303113    5632 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0110 04:52:36.306534    5632 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0110 04:52:36.309988    5632 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0110 04:52:36.313192    5632 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0110 04:52:36.316406    5632 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0110 04:52:36.319651    5632 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0110 04:52:36.322692    5632 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0110 04:52:36.408959    5632 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0110 04:52:36.418987    5632 start.go:495] detecting cgroup driver to use...
I0110 04:52:36.419419    5632 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0110 04:52:36.424927    5632 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I0110 04:52:36.434680    5632 ssh_runner.go:195] Run: sudo systemctl stop -f containerd
I0110 04:52:36.441957    5632 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I0110 04:52:36.447038    5632 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0110 04:52:36.451732    5632 ssh_runner.go:195] Run: sudo systemctl stop -f crio
I0110 04:52:36.499602    5632 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0110 04:52:36.505712    5632 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0110 04:52:36.512453    5632 ssh_runner.go:195] Run: which cri-dockerd
I0110 04:52:36.513850    5632 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0110 04:52:36.517092    5632 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0110 04:52:36.522376    5632 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0110 04:52:36.587071    5632 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0110 04:52:36.656146    5632 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0110 04:52:36.656311    5632 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0110 04:52:36.662317    5632 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0110 04:52:36.740338    5632 ssh_runner.go:195] Run: sudo systemctl restart docker
W0110 04:52:37.253594    5632 out.go:270] ❗  Failing to connect to https://registry.k8s.io/ from inside the minikube VM
W0110 04:52:37.253644    5632 out.go:270] 💡  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0110 04:52:39.000600    5632 ssh_runner.go:235] Completed: sudo systemctl restart docker: (2.260272875s)
I0110 04:52:39.001086    5632 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0110 04:52:39.006628    5632 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0110 04:52:39.011661    5632 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0110 04:52:39.090846    5632 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0110 04:52:39.166861    5632 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0110 04:52:39.233951    5632 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0110 04:52:39.240267    5632 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0110 04:52:39.245259    5632 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0110 04:52:39.314735    5632 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0110 04:52:39.337990    5632 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0110 04:52:39.338390    5632 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0110 04:52:39.340323    5632 start.go:563] Will wait 60s for crictl version
I0110 04:52:39.340576    5632 ssh_runner.go:195] Run: which crictl
I0110 04:52:39.342075    5632 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0110 04:52:39.355379    5632 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.2.0
RuntimeApiVersion:  v1
I0110 04:52:39.356150    5632 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0110 04:52:39.364635    5632 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0110 04:52:39.386644    5632 out.go:235] 🐳  Preparing Kubernetes v1.31.0 on Docker 27.2.0 ...
I0110 04:52:39.654537    5632 ssh_runner.go:195] Run: grep 10.211.55.2	host.minikube.internal$ /etc/hosts
I0110 04:52:39.656539    5632 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "10.211.55.2	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0110 04:52:39.660786    5632 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.34.0-arm64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:6000 CPUs:2 DiskSize:20000 Driver:parallels HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:10.211.55.4 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0110 04:52:39.660849    5632 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I0110 04:52:39.661248    5632 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0110 04:52:39.668553    5632 docker.go:685] Got preloaded images: -- stdout --
getmeili/meilisearch:latest
jaegertracing/all-in-one:latest
redis:latest
prom/prometheus:latest
getmeili/meilisearch:<none>
localhost/reactory/reactory-express-server:1.0.0
mongo:latest
grafana/grafana:latest
nginx:latest
postgres:latest
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
gcr.io/k8s-minikube/storage-provisioner:v5
nginx:1.14.2

-- /stdout --
I0110 04:52:39.668558    5632 docker.go:615] Images already preloaded, skipping extraction
I0110 04:52:39.668939    5632 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0110 04:52:39.675043    5632 docker.go:685] Got preloaded images: -- stdout --
getmeili/meilisearch:latest
jaegertracing/all-in-one:latest
redis:latest
prom/prometheus:latest
getmeili/meilisearch:<none>
localhost/reactory/reactory-express-server:1.0.0
mongo:latest
grafana/grafana:latest
nginx:latest
postgres:latest
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
gcr.io/k8s-minikube/storage-provisioner:v5
nginx:1.14.2

-- /stdout --
I0110 04:52:39.675048    5632 cache_images.go:84] Images are preloaded, skipping loading
I0110 04:52:39.675073    5632 kubeadm.go:934] updating node { 10.211.55.4 8443 v1.31.0 docker true true} ...
I0110 04:52:39.675143    5632 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.31.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=10.211.55.4

[Install]
 config:
{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0110 04:52:39.675547    5632 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0110 04:52:39.693244    5632 cni.go:84] Creating CNI manager for ""
I0110 04:52:39.693250    5632 cni.go:158] "parallels" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0110 04:52:39.693254    5632 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0110 04:52:39.693263    5632 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:10.211.55.4 APIServerPort:8443 KubernetesVersion:v1.31.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "10.211.55.4"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:10.211.55.4 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0110 04:52:39.693349    5632 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 10.211.55.4
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 10.211.55.4
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "10.211.55.4"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.31.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0110 04:52:39.693522    5632 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.31.0
I0110 04:52:39.697018    5632 binaries.go:44] Found k8s binaries, skipping transfer
I0110 04:52:39.697135    5632 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0110 04:52:39.700628    5632 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (306 bytes)
I0110 04:52:39.706169    5632 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0110 04:52:39.711713    5632 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2147 bytes)
I0110 04:52:39.717782    5632 ssh_runner.go:195] Run: grep 10.211.55.4	control-plane.minikube.internal$ /etc/hosts
I0110 04:52:39.718883    5632 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "10.211.55.4	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0110 04:52:39.722668    5632 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0110 04:52:39.800835    5632 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0110 04:52:39.807545    5632 certs.go:68] Setting up /Users/wweber/.minikube/profiles/minikube for IP: 10.211.55.4
I0110 04:52:39.807563    5632 certs.go:194] generating shared ca certs ...
I0110 04:52:39.807602    5632 certs.go:226] acquiring lock for ca certs: {Name:mk96b4587e188ffa23a772a5e5543521abe39856 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0110 04:52:39.808453    5632 certs.go:235] skipping valid "minikubeCA" ca cert: /Users/wweber/.minikube/ca.key
I0110 04:52:39.809591    5632 certs.go:235] skipping valid "proxyClientCA" ca cert: /Users/wweber/.minikube/proxy-client-ca.key
I0110 04:52:39.809619    5632 certs.go:256] generating profile certs ...
I0110 04:52:39.810044    5632 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /Users/wweber/.minikube/profiles/minikube/client.key
I0110 04:52:39.810529    5632 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /Users/wweber/.minikube/profiles/minikube/apiserver.key.d2b317b8
I0110 04:52:39.811064    5632 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /Users/wweber/.minikube/profiles/minikube/proxy-client.key
I0110 04:52:39.811932    5632 certs.go:484] found cert: /Users/wweber/.minikube/certs/ca-key.pem (1675 bytes)
I0110 04:52:39.812054    5632 certs.go:484] found cert: /Users/wweber/.minikube/certs/ca.pem (1078 bytes)
I0110 04:52:39.812187    5632 certs.go:484] found cert: /Users/wweber/.minikube/certs/cert.pem (1123 bytes)
I0110 04:52:39.812303    5632 certs.go:484] found cert: /Users/wweber/.minikube/certs/key.pem (1675 bytes)
I0110 04:52:39.813141    5632 ssh_runner.go:362] scp /Users/wweber/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0110 04:52:39.823004    5632 ssh_runner.go:362] scp /Users/wweber/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0110 04:52:39.831515    5632 ssh_runner.go:362] scp /Users/wweber/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0110 04:52:39.838680    5632 ssh_runner.go:362] scp /Users/wweber/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0110 04:52:39.846539    5632 ssh_runner.go:362] scp /Users/wweber/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0110 04:52:39.854396    5632 ssh_runner.go:362] scp /Users/wweber/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0110 04:52:39.862049    5632 ssh_runner.go:362] scp /Users/wweber/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0110 04:52:39.870795    5632 ssh_runner.go:362] scp /Users/wweber/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0110 04:52:39.878377    5632 ssh_runner.go:362] scp /Users/wweber/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0110 04:52:39.887174    5632 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (740 bytes)
I0110 04:52:39.892805    5632 ssh_runner.go:195] Run: openssl version
I0110 04:52:39.895312    5632 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0110 04:52:39.899350    5632 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0110 04:52:39.906334    5632 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Dec 27 16:49 /usr/share/ca-certificates/minikubeCA.pem
I0110 04:52:39.906681    5632 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0110 04:52:39.908650    5632 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0110 04:52:39.912277    5632 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0110 04:52:39.913985    5632 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0110 04:52:39.915936    5632 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0110 04:52:39.917925    5632 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0110 04:52:39.919870    5632 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0110 04:52:39.921769    5632 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0110 04:52:39.923556    5632 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0110 04:52:39.925551    5632 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.34.0-arm64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:6000 CPUs:2 DiskSize:20000 Driver:parallels HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:10.211.55.4 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0110 04:52:39.925961    5632 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0110 04:52:39.937888    5632 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0110 04:52:39.941123    5632 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I0110 04:52:39.941166    5632 kubeadm.go:593] restartPrimaryControlPlane start ...
I0110 04:52:39.941351    5632 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0110 04:52:39.944299    5632 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0110 04:52:39.945630    5632 kubeconfig.go:47] verify endpoint returned: get endpoint: "minikube" does not appear in /Users/wweber/.kube/config
I0110 04:52:39.946090    5632 kubeconfig.go:62] /Users/wweber/.kube/config needs updating (will repair): [kubeconfig missing "minikube" cluster setting kubeconfig missing "minikube" context setting]
I0110 04:52:39.946599    5632 lock.go:35] WriteFile acquiring /Users/wweber/.kube/config: {Name:mkeac55d52a7b3f517dcc35725f96a0067a57e39 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0110 04:52:39.951154    5632 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0110 04:52:39.954185    5632 kubeadm.go:630] The running cluster does not require reconfiguration: 10.211.55.4
I0110 04:52:39.954198    5632 kubeadm.go:1160] stopping kube-system containers ...
I0110 04:52:39.954467    5632 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0110 04:52:39.961216    5632 docker.go:483] Stopping containers: [b21336b7b902 bf31ba2912e3 60db049aee62 a28e6a7e5dd4 edabe9067042 1408dcedf4fa b16f2bdb58f2 6d00c4f8cb65 2462b115d9cb 38dadbda977e c29eb56835d2 523b538e1f9b 82749c02569f 4ac5d1e5db73 4d4b15c3e5a0 3f93ab6c7536 3472c08682e7 3f6f27a6d807 cdaea275aa99 221aba2a713d 0c26c9a20273 9fd57dfd7e1b f80e871988dc 5f5793ee1c07 2aa3dadc73c8 d2be98106172 5f68fb035845 38c59cd7bfc1]
I0110 04:52:39.961551    5632 ssh_runner.go:195] Run: docker stop b21336b7b902 bf31ba2912e3 60db049aee62 a28e6a7e5dd4 edabe9067042 1408dcedf4fa b16f2bdb58f2 6d00c4f8cb65 2462b115d9cb 38dadbda977e c29eb56835d2 523b538e1f9b 82749c02569f 4ac5d1e5db73 4d4b15c3e5a0 3f93ab6c7536 3472c08682e7 3f6f27a6d807 cdaea275aa99 221aba2a713d 0c26c9a20273 9fd57dfd7e1b f80e871988dc 5f5793ee1c07 2aa3dadc73c8 d2be98106172 5f68fb035845 38c59cd7bfc1
I0110 04:52:39.968131    5632 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0110 04:52:39.974254    5632 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0110 04:52:39.977220    5632 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0110 04:52:39.977229    5632 kubeadm.go:157] found existing configuration files:

I0110 04:52:39.977431    5632 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0110 04:52:39.980264    5632 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0110 04:52:39.980467    5632 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0110 04:52:39.983556    5632 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0110 04:52:39.986506    5632 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0110 04:52:39.986745    5632 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0110 04:52:39.989993    5632 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0110 04:52:39.993035    5632 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0110 04:52:39.993318    5632 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0110 04:52:39.996607    5632 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0110 04:52:39.999729    5632 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0110 04:52:39.999974    5632 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0110 04:52:40.003298    5632 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0110 04:52:40.006535    5632 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.31.0:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0110 04:52:40.047842    5632 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.31.0:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0110 04:52:40.680359    5632 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.31.0:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0110 04:52:40.802701    5632 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.31.0:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0110 04:52:40.825955    5632 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.31.0:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0110 04:52:40.847712    5632 api_server.go:52] waiting for apiserver process to appear ...
I0110 04:52:40.848072    5632 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0110 04:52:41.349505    5632 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0110 04:52:41.849588    5632 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0110 04:52:42.349317    5632 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0110 04:52:42.354595    5632 api_server.go:72] duration metric: took 1.506912375s to wait for apiserver process to appear ...
I0110 04:52:42.354610    5632 api_server.go:88] waiting for apiserver healthz status ...
I0110 04:52:42.354657    5632 api_server.go:253] Checking apiserver healthz at https://10.211.55.4:8443/healthz ...
I0110 04:52:44.646505    5632 api_server.go:279] https://10.211.55.4:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0110 04:52:44.646545    5632 api_server.go:103] status: https://10.211.55.4:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0110 04:52:44.646599    5632 api_server.go:253] Checking apiserver healthz at https://10.211.55.4:8443/healthz ...
I0110 04:52:44.657162    5632 api_server.go:279] https://10.211.55.4:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0110 04:52:44.657175    5632 api_server.go:103] status: https://10.211.55.4:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0110 04:52:44.855784    5632 api_server.go:253] Checking apiserver healthz at https://10.211.55.4:8443/healthz ...
I0110 04:52:44.861096    5632 api_server.go:279] https://10.211.55.4:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0110 04:52:44.861115    5632 api_server.go:103] status: https://10.211.55.4:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0110 04:52:45.355648    5632 api_server.go:253] Checking apiserver healthz at https://10.211.55.4:8443/healthz ...
I0110 04:52:45.358685    5632 api_server.go:279] https://10.211.55.4:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0110 04:52:45.358692    5632 api_server.go:103] status: https://10.211.55.4:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0110 04:52:45.855426    5632 api_server.go:253] Checking apiserver healthz at https://10.211.55.4:8443/healthz ...
I0110 04:52:45.864075    5632 api_server.go:279] https://10.211.55.4:8443/healthz returned 200:
ok
I0110 04:52:45.879256    5632 api_server.go:141] control plane version: v1.31.0
I0110 04:52:45.879395    5632 api_server.go:131] duration metric: took 3.524819625s to wait for apiserver health ...
I0110 04:52:45.879406    5632 cni.go:84] Creating CNI manager for ""
I0110 04:52:45.879477    5632 cni.go:158] "parallels" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0110 04:52:45.884080    5632 out.go:177] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I0110 04:52:45.888026    5632 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0110 04:52:45.893592    5632 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0110 04:52:45.900480    5632 system_pods.go:43] waiting for kube-system pods to appear ...
I0110 04:52:45.904307    5632 system_pods.go:59] 7 kube-system pods found
I0110 04:52:45.904315    5632 system_pods.go:61] "coredns-6f6b679f8f-gdpck" [38d9d2f7-9fa9-4858-b5a6-0c60d654b104] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0110 04:52:45.904319    5632 system_pods.go:61] "etcd-minikube" [da04831f-384d-4101-b2c7-42d997fa081e] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0110 04:52:45.904323    5632 system_pods.go:61] "kube-apiserver-minikube" [fbb402e3-8473-479f-96eb-cecc870c9f41] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0110 04:52:45.904325    5632 system_pods.go:61] "kube-controller-manager-minikube" [54aeddcc-8260-4b92-b578-c3dff094c718] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0110 04:52:45.904327    5632 system_pods.go:61] "kube-proxy-v8z2d" [86f711f6-de4e-4f85-b02a-4b58a5730120] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0110 04:52:45.904329    5632 system_pods.go:61] "kube-scheduler-minikube" [0d479ff4-217f-453c-98af-1ecb47bd87db] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0110 04:52:45.904330    5632 system_pods.go:61] "storage-provisioner" [ed0ccca3-ba57-4699-86c4-a5a0a5379443] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0110 04:52:45.904332    5632 system_pods.go:74] duration metric: took 3.848666ms to wait for pod list to return data ...
I0110 04:52:45.904335    5632 node_conditions.go:102] verifying NodePressure condition ...
I0110 04:52:45.905687    5632 node_conditions.go:122] node storage ephemeral capacity is 17734596Ki
I0110 04:52:45.905692    5632 node_conditions.go:123] node cpu capacity is 2
I0110 04:52:45.905697    5632 node_conditions.go:105] duration metric: took 1.359875ms to run NodePressure ...
I0110 04:52:45.905703    5632 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.31.0:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0110 04:52:46.135842    5632 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0110 04:52:46.142408    5632 ops.go:34] apiserver oom_adj: -16
I0110 04:52:46.142421    5632 kubeadm.go:597] duration metric: took 6.201366791s to restartPrimaryControlPlane
I0110 04:52:46.142440    5632 kubeadm.go:394] duration metric: took 6.217014917s to StartCluster
I0110 04:52:46.142465    5632 settings.go:142] acquiring lock: {Name:mk46a590a9f293a7011cccea1930f76905682371 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0110 04:52:46.143090    5632 settings.go:150] Updating kubeconfig:  /Users/wweber/.kube/config
I0110 04:52:46.144976    5632 lock.go:35] WriteFile acquiring /Users/wweber/.kube/config: {Name:mkeac55d52a7b3f517dcc35725f96a0067a57e39 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0110 04:52:46.145822    5632 start.go:235] Will wait 6m0s for node &{Name: IP:10.211.55.4 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0110 04:52:46.145879    5632 addons.go:507] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0110 04:52:46.145953    5632 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0110 04:52:46.145969    5632 addons.go:234] Setting addon storage-provisioner=true in "minikube"
W0110 04:52:46.145977    5632 addons.go:243] addon storage-provisioner should already be in state true
I0110 04:52:46.146003    5632 host.go:66] Checking if "minikube" exists ...
I0110 04:52:46.146033    5632 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0110 04:52:46.146091    5632 config.go:182] Loaded profile config "minikube": Driver=parallels, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I0110 04:52:46.146069    5632 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0110 04:52:46.146676    5632 main.go:141] libmachine: executing: /usr/local/bin/prlctl list minikube --output status --no-header
I0110 04:52:46.147117    5632 main.go:141] libmachine: executing: /usr/local/bin/prlctl list minikube --output status --no-header
I0110 04:52:46.154365    5632 out.go:177] 🔎  Verifying Kubernetes components...
I0110 04:52:46.158569    5632 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0110 04:52:46.277551    5632 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0110 04:52:46.286612    5632 api_server.go:52] waiting for apiserver process to appear ...
I0110 04:52:46.286722    5632 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0110 04:52:46.292354    5632 api_server.go:72] duration metric: took 146.514375ms to wait for apiserver process to appear ...
I0110 04:52:46.292359    5632 api_server.go:88] waiting for apiserver healthz status ...
I0110 04:52:46.292366    5632 api_server.go:253] Checking apiserver healthz at https://10.211.55.4:8443/healthz ...
I0110 04:52:46.294631    5632 api_server.go:279] https://10.211.55.4:8443/healthz returned 200:
ok
I0110 04:52:46.295073    5632 api_server.go:141] control plane version: v1.31.0
I0110 04:52:46.295077    5632 api_server.go:131] duration metric: took 2.715584ms to wait for apiserver health ...
I0110 04:52:46.295080    5632 system_pods.go:43] waiting for kube-system pods to appear ...
I0110 04:52:46.297420    5632 system_pods.go:59] 7 kube-system pods found
I0110 04:52:46.297433    5632 system_pods.go:61] "coredns-6f6b679f8f-gdpck" [38d9d2f7-9fa9-4858-b5a6-0c60d654b104] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0110 04:52:46.297440    5632 system_pods.go:61] "etcd-minikube" [da04831f-384d-4101-b2c7-42d997fa081e] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0110 04:52:46.297442    5632 system_pods.go:61] "kube-apiserver-minikube" [fbb402e3-8473-479f-96eb-cecc870c9f41] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0110 04:52:46.297445    5632 system_pods.go:61] "kube-controller-manager-minikube" [54aeddcc-8260-4b92-b578-c3dff094c718] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0110 04:52:46.297446    5632 system_pods.go:61] "kube-proxy-v8z2d" [86f711f6-de4e-4f85-b02a-4b58a5730120] Running
I0110 04:52:46.297448    5632 system_pods.go:61] "kube-scheduler-minikube" [0d479ff4-217f-453c-98af-1ecb47bd87db] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0110 04:52:46.297448    5632 system_pods.go:61] "storage-provisioner" [ed0ccca3-ba57-4699-86c4-a5a0a5379443] Running
I0110 04:52:46.297450    5632 system_pods.go:74] duration metric: took 2.36825ms to wait for pod list to return data ...
I0110 04:52:46.297454    5632 kubeadm.go:582] duration metric: took 151.614583ms to wait for: map[apiserver:true system_pods:true]
I0110 04:52:46.297459    5632 node_conditions.go:102] verifying NodePressure condition ...
I0110 04:52:46.298295    5632 node_conditions.go:122] node storage ephemeral capacity is 17734596Ki
I0110 04:52:46.298298    5632 node_conditions.go:123] node cpu capacity is 2
I0110 04:52:46.298303    5632 node_conditions.go:105] duration metric: took 842.833µs to run NodePressure ...
I0110 04:52:46.298307    5632 start.go:241] waiting for startup goroutines ...
I0110 04:52:46.345922    5632 addons.go:234] Setting addon default-storageclass=true in "minikube"
W0110 04:52:46.345927    5632 addons.go:243] addon default-storageclass should already be in state true
I0110 04:52:46.345943    5632 host.go:66] Checking if "minikube" exists ...
I0110 04:52:46.346195    5632 main.go:141] libmachine: executing: /usr/local/bin/prlctl list minikube --output status --no-header
I0110 04:52:46.349797    5632 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0110 04:52:46.353989    5632 addons.go:431] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0110 04:52:46.353993    5632 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0110 04:52:46.354014    5632 main.go:141] libmachine: executing: /usr/local/bin/prlctl list minikube --output status --no-header
I0110 04:52:46.499124    5632 addons.go:431] installing /etc/kubernetes/addons/storageclass.yaml
I0110 04:52:46.499139    5632 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0110 04:52:46.499164    5632 main.go:141] libmachine: executing: /usr/local/bin/prlctl list minikube --output status --no-header
I0110 04:52:46.505370    5632 main.go:141] libmachine: executing: /usr/local/bin/prlctl list -i minikube
I0110 04:52:46.674669    5632 main.go:141] libmachine: executing: /usr/local/bin/prlctl list -i minikube
I0110 04:52:46.704955    5632 main.go:141] libmachine: Found lease: 10.211.55.4 for MAC: 001C42B2DA18, expiring at 1736479340, leased for 1800 s.

I0110 04:52:46.704982    5632 main.go:141] libmachine: Found IP lease: 10.211.55.4 for MAC address 001C42B2DA18

I0110 04:52:46.704988    5632 sshutil.go:53] new ssh client: &{IP:10.211.55.4 Port:22 SSHKeyPath:/Users/wweber/.minikube/machines/minikube/id_rsa Username:docker}
I0110 04:52:46.731953    5632 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0110 04:52:46.841417    5632 main.go:141] libmachine: Found lease: 10.211.55.4 for MAC: 001C42B2DA18, expiring at 1736479340, leased for 1800 s.

I0110 04:52:46.841424    5632 main.go:141] libmachine: Found IP lease: 10.211.55.4 for MAC address 001C42B2DA18

I0110 04:52:46.841430    5632 sshutil.go:53] new ssh client: &{IP:10.211.55.4 Port:22 SSHKeyPath:/Users/wweber/.minikube/machines/minikube/id_rsa Username:docker}
I0110 04:52:46.874604    5632 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0110 04:52:47.139642    5632 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass
I0110 04:52:47.143522    5632 addons.go:510] duration metric: took 997.704292ms for enable addons: enabled=[storage-provisioner default-storageclass]
I0110 04:52:47.143583    5632 start.go:246] waiting for cluster config update ...
I0110 04:52:47.143607    5632 start.go:255] writing updated cluster config ...
I0110 04:52:47.145488    5632 ssh_runner.go:195] Run: rm -f paused
I0110 04:52:47.284005    5632 start.go:600] kubectl: 1.32.0, cluster: 1.31.0 (minor skew: 1)
I0110 04:52:47.288609    5632 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Jan 10 23:34:37 minikube dockerd[907]: time="2025-01-10T23:34:37.365814873Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 10 23:34:37 minikube dockerd[907]: time="2025-01-10T23:34:37.366227496Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 10 23:34:37 minikube dockerd[907]: time="2025-01-10T23:34:37.371916343Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 10 23:34:57 minikube dockerd[907]: time="2025-01-10T23:34:57.353135104Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 10 23:34:57 minikube dockerd[907]: time="2025-01-10T23:34:57.353488977Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 10 23:34:57 minikube dockerd[907]: time="2025-01-10T23:34:57.356551422Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 10 23:39:40 minikube dockerd[907]: time="2025-01-10T23:39:40.363719468Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 10 23:39:40 minikube dockerd[907]: time="2025-01-10T23:39:40.363974468Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 10 23:39:40 minikube dockerd[907]: time="2025-01-10T23:39:40.366953510Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 10 23:40:07 minikube dockerd[907]: time="2025-01-10T23:40:07.363898121Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 10 23:40:07 minikube dockerd[907]: time="2025-01-10T23:40:07.364065830Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 10 23:40:07 minikube dockerd[907]: time="2025-01-10T23:40:07.366875373Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 10 23:44:46 minikube dockerd[907]: time="2025-01-10T23:44:46.381662562Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 10 23:44:46 minikube dockerd[907]: time="2025-01-10T23:44:46.381850230Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 10 23:44:46 minikube dockerd[907]: time="2025-01-10T23:44:46.386613703Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 10 23:45:10 minikube dockerd[907]: time="2025-01-10T23:45:10.346505706Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 10 23:45:10 minikube dockerd[907]: time="2025-01-10T23:45:10.346619165Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 10 23:45:10 minikube dockerd[907]: time="2025-01-10T23:45:10.349006298Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 10 23:50:04 minikube dockerd[907]: time="2025-01-10T23:50:04.348069897Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 10 23:50:04 minikube dockerd[907]: time="2025-01-10T23:50:04.348212440Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 10 23:50:04 minikube dockerd[907]: time="2025-01-10T23:50:04.354186866Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 10 23:50:17 minikube dockerd[907]: time="2025-01-10T23:50:17.341758852Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 10 23:50:17 minikube dockerd[907]: time="2025-01-10T23:50:17.341855561Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 10 23:50:17 minikube dockerd[907]: time="2025-01-10T23:50:17.344268040Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 10 23:55:10 minikube dockerd[907]: time="2025-01-10T23:55:10.382169229Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 10 23:55:10 minikube dockerd[907]: time="2025-01-10T23:55:10.382240688Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 10 23:55:10 minikube dockerd[907]: time="2025-01-10T23:55:10.385186544Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 10 23:55:28 minikube dockerd[907]: time="2025-01-10T23:55:28.329318152Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 10 23:55:28 minikube dockerd[907]: time="2025-01-10T23:55:28.329481570Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 10 23:55:28 minikube dockerd[907]: time="2025-01-10T23:55:28.331857130Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 11 00:00:18 minikube dockerd[907]: time="2025-01-11T00:00:18.350909193Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 11 00:00:18 minikube dockerd[907]: time="2025-01-11T00:00:18.350997819Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 11 00:00:18 minikube dockerd[907]: time="2025-01-11T00:00:18.352960542Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 11 00:00:37 minikube dockerd[907]: time="2025-01-11T00:00:37.488668483Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 11 00:00:37 minikube dockerd[907]: time="2025-01-11T00:00:37.489199278Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 11 00:00:37 minikube dockerd[907]: time="2025-01-11T00:00:37.493635978Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 11 00:05:28 minikube dockerd[907]: time="2025-01-11T00:05:28.361478590Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 11 00:05:28 minikube dockerd[907]: time="2025-01-11T00:05:28.361728966Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 11 00:05:28 minikube dockerd[907]: time="2025-01-11T00:05:28.364411647Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 11 00:05:41 minikube dockerd[907]: time="2025-01-11T00:05:41.377374593Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 11 00:05:41 minikube dockerd[907]: time="2025-01-11T00:05:41.378842726Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 11 00:05:41 minikube dockerd[907]: time="2025-01-11T00:05:41.382672705Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 11 00:10:29 minikube dockerd[907]: time="2025-01-11T00:10:29.327133419Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 11 00:10:29 minikube dockerd[907]: time="2025-01-11T00:10:29.328441593Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 11 00:10:29 minikube dockerd[907]: time="2025-01-11T00:10:29.338167193Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 11 00:10:46 minikube dockerd[907]: time="2025-01-11T00:10:46.364343760Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 11 00:10:46 minikube dockerd[907]: time="2025-01-11T00:10:46.365451475Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 11 00:10:46 minikube dockerd[907]: time="2025-01-11T00:10:46.369400665Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 11 00:15:34 minikube dockerd[907]: time="2025-01-11T00:15:34.375580671Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 11 00:15:34 minikube dockerd[907]: time="2025-01-11T00:15:34.377509558Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 11 00:15:34 minikube dockerd[907]: time="2025-01-11T00:15:34.382324546Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 11 00:15:58 minikube dockerd[907]: time="2025-01-11T00:15:58.398697316Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 11 00:15:58 minikube dockerd[907]: time="2025-01-11T00:15:58.398761025Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 11 00:15:58 minikube dockerd[907]: time="2025-01-11T00:15:58.401610002Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 11 00:20:40 minikube dockerd[907]: time="2025-01-11T00:20:40.350616470Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 11 00:20:40 minikube dockerd[907]: time="2025-01-11T00:20:40.350753929Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 11 00:20:40 minikube dockerd[907]: time="2025-01-11T00:20:40.360420396Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 11 00:21:01 minikube dockerd[907]: time="2025-01-11T00:21:01.350042361Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 11 00:21:01 minikube dockerd[907]: time="2025-01-11T00:21:01.350241196Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jan 11 00:21:01 minikube dockerd[907]: time="2025-01-11T00:21:01.353513796Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"


==> container status <==
CONTAINER           IMAGE                                                                                              CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
3f3c5214faa66       ba04bb24b9575                                                                                      2 hours ago         Running             storage-provisioner       45                  9c05002a4008b       storage-provisioner
b78d6302ec408       ba04bb24b9575                                                                                      2 hours ago         Exited              storage-provisioner       44                  9c05002a4008b       storage-provisioner
68827ac77cf90       redis@sha256:cd13b924409d740ea8abe6677a7d1accf696898408d330a3d7c8234fa7545775                      9 hours ago         Running             redis                     0                   74620ef6c68ff       reactory-redis-6658b5b567-xxl6s
a7a2fd39d82e7       prom/prometheus@sha256:6559acbd5d770b15bb3c954629ce190ac3cbbdb2b7f1c30f0385c4e05104e218            10 hours ago        Running             prometheus                0                   ed493642eced7       reactory-prometheus-57dcc99f-dnd5r
90b5b3e52a7b2       getmeili/meilisearch@sha256:422528858432dd454d18a0c99fa501b82789a43b321551340421c14685832d6c       21 hours ago        Running             meilisearch               2                   1ae17228cf667       reactory-meilisearch-577c78589d-dczt5
22f899fdbc539       mongo@sha256:4f93a84f7d4d8b1b6cb7e0c172d8a44b0bed9b399f207165ea19473bdb5a36b0                      21 hours ago        Running             mongodb                   4                   d0905e96201be       reactory-mongodb-5bf9f5d545-hnh7z
306631cac81d6       nginx@sha256:42e917aaa1b5bb40dd0f6f7f4f857490ac7747d7ef73b391c774a41a8b994f15                      21 hours ago        Running             nginx                     4                   be8524da9c9b6       reactory-nginx-66f8c7d4c5-9hlvd
74df32e9a1f8d       jaegertracing/all-in-one@sha256:12fa17a231abded2c3b5b715bd252a043678495c588cbe772173991fbdcdf7c8   21 hours ago        Running             jaeger                    2                   6053949e6e5f5       reactory-jaeger-7bd97f4cfc-894dg
67b3a3e4b69d9       grafana/grafana@sha256:d8ea37798ccc41061a62ab080f2676dda6bf7815558499f901bdb0f533a456fb            21 hours ago        Running             grafana                   2                   31d40f41e4f20       reactory-grafana-8f6f95f7c-9vtb8
f6ded36b6a277       postgres@sha256:888402a8cd6075c5dc83a31f58287f13306c318eaad016661ed12e076f3e6341                   21 hours ago        Running             postgres                  2                   0667db90e3eb7       reactory-postgres-5d56c6d77d-dq7v7
8a564140864fc       7bbc8783b8ecf                                                                                      21 hours ago        Running             nginx                     2                   0e65bb7cbd9f3       reactory-nginx-6ddc48c85b-kmmgz
33a38da603300       7bbc8783b8ecf                                                                                      21 hours ago        Running             nginx                     2                   4f13d15a53bc9       reactory-pwa-client-5bf5bdf4bf-t7hp9
e6964545ad23d       7bbc8783b8ecf                                                                                      21 hours ago        Running             nginx                     2                   cf963dbcafae7       reactory-pwa-client-5bf5bdf4bf-5kvqj
dcb5e6aade36f       2437cf7621777                                                                                      21 hours ago        Running             coredns                   5                   5313b910a9d61       coredns-6f6b679f8f-gdpck
aef0dc4806606       7bbc8783b8ecf                                                                                      21 hours ago        Running             nginx                     2                   ea70f18517da3       reactory-nginx-6ddc48c85b-c2x8s
126cefdaf2ed7       71d55d66fd4ee                                                                                      21 hours ago        Running             kube-proxy                5                   7995e2947e7ee       kube-proxy-v8z2d
a2fc778192c3e       fbbbd428abb4d                                                                                      21 hours ago        Running             kube-scheduler            5                   b86ad42cf74b4       kube-scheduler-minikube
873cff1dde2a7       27e3830e14027                                                                                      21 hours ago        Running             etcd                      5                   ca741bdbdf9e9       etcd-minikube
69e63a48845db       cd0f0ae0ec9e0                                                                                      21 hours ago        Running             kube-apiserver            6                   f84e0b9856484       kube-apiserver-minikube
17606f5b8c98b       fcb0683e6bdbd                                                                                      21 hours ago        Running             kube-controller-manager   6                   3b69053e3389e       kube-controller-manager-minikube
51a05f9e4e4fd       mongo@sha256:4f93a84f7d4d8b1b6cb7e0c172d8a44b0bed9b399f207165ea19473bdb5a36b0                      23 hours ago        Exited              mongodb                   3                   587d70c57b931       reactory-mongodb-5bf9f5d545-hnh7z
a185c6ac57c50       nginx@sha256:42e917aaa1b5bb40dd0f6f7f4f857490ac7747d7ef73b391c774a41a8b994f15                      23 hours ago        Exited              nginx                     3                   25098126d49d8       reactory-nginx-66f8c7d4c5-9hlvd
9be544e01b5be       postgres@sha256:888402a8cd6075c5dc83a31f58287f13306c318eaad016661ed12e076f3e6341                   23 hours ago        Exited              postgres                  1                   0a92d9f28c28c       reactory-postgres-5d56c6d77d-dq7v7
fd1e5905ec868       jaegertracing/all-in-one@sha256:12fa17a231abded2c3b5b715bd252a043678495c588cbe772173991fbdcdf7c8   23 hours ago        Exited              jaeger                    1                   fa520c6832c18       reactory-jaeger-7bd97f4cfc-894dg
180fe8f50a040       getmeili/meilisearch@sha256:422528858432dd454d18a0c99fa501b82789a43b321551340421c14685832d6c       23 hours ago        Exited              meilisearch               1                   357ebd2b7024f       reactory-meilisearch-577c78589d-dczt5
8a4de41b74f2e       grafana/grafana@sha256:d8ea37798ccc41061a62ab080f2676dda6bf7815558499f901bdb0f533a456fb            23 hours ago        Exited              grafana                   1                   4ca7960b86ffc       reactory-grafana-8f6f95f7c-9vtb8
ca79f117a7898       7bbc8783b8ecf                                                                                      23 hours ago        Exited              nginx                     1                   e9bd43a46f4f9       reactory-nginx-6ddc48c85b-kmmgz
f67e06e88218a       7bbc8783b8ecf                                                                                      23 hours ago        Exited              nginx                     1                   6d499e6cd0116       reactory-nginx-6ddc48c85b-c2x8s
b21336b7b9023       2437cf7621777                                                                                      23 hours ago        Exited              coredns                   4                   a28e6a7e5dd40       coredns-6f6b679f8f-gdpck
097380f7e6beb       7bbc8783b8ecf                                                                                      23 hours ago        Exited              nginx                     1                   fd9b3b81a7c33       reactory-pwa-client-5bf5bdf4bf-5kvqj
bf31ba2912e3a       71d55d66fd4ee                                                                                      23 hours ago        Exited              kube-proxy                4                   1408dcedf4fa2       kube-proxy-v8z2d
fe230b210df69       7bbc8783b8ecf                                                                                      23 hours ago        Exited              nginx                     1                   b114210003e17       reactory-pwa-client-5bf5bdf4bf-t7hp9
b16f2bdb58f2c       27e3830e14027                                                                                      23 hours ago        Exited              etcd                      4                   6d00c4f8cb651       etcd-minikube
2462b115d9cb8       fcb0683e6bdbd                                                                                      23 hours ago        Exited              kube-controller-manager   5                   c29eb56835d2a       kube-controller-manager-minikube
38dadbda977ed       cd0f0ae0ec9e0                                                                                      23 hours ago        Exited              kube-apiserver            5                   523b538e1f9b2       kube-apiserver-minikube
82749c02569f2       fbbbd428abb4d                                                                                      23 hours ago        Exited              kube-scheduler            4                   4ac5d1e5db734       kube-scheduler-minikube


==> coredns [b21336b7b902] <==
[INFO] 10.244.0.68:48992 - 47801 "AAAA IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000076904s
[INFO] 10.244.0.68:58839 - 33171 "A IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000060157s
[INFO] 10.244.0.68:46381 - 64600 "A IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 56 0.049961612s
[INFO] 10.244.0.68:37457 - 39984 "AAAA IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 29 0.050326801s
[INFO] 10.244.0.68:46677 - 17911 "A IN grafana.com.reactory.svc.cluster.local. udp 67 false 1232" NXDOMAIN qr,aa,rd 149 0.000539928s
[INFO] 10.244.0.68:46154 - 53971 "AAAA IN grafana.com.reactory.svc.cluster.local. udp 67 false 1232" NXDOMAIN qr,aa,rd 149 0.000526762s
[INFO] 10.244.0.68:34101 - 15146 "A IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000193796s
[INFO] 10.244.0.68:60989 - 16442 "AAAA IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000267131s
[INFO] 10.244.0.68:40252 - 63845 "AAAA IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000321674s
[INFO] 10.244.0.68:59641 - 64012 "A IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000506594s
[INFO] 10.244.0.68:53597 - 60797 "A IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 56 0.036699609s
[INFO] 10.244.0.68:37782 - 16526 "AAAA IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 29 0.036367644s
[INFO] 10.244.0.68:54561 - 53576 "A IN grafana.com.reactory.svc.cluster.local. udp 67 false 1232" NXDOMAIN qr,aa,rd 149 0.001216466s
[INFO] 10.244.0.68:34129 - 59212 "AAAA IN grafana.com.reactory.svc.cluster.local. udp 67 false 1232" NXDOMAIN qr,aa,rd 149 0.001293797s
[INFO] 10.244.0.68:60130 - 6571 "A IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000321574s
[INFO] 10.244.0.68:44825 - 60075 "AAAA IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000166995s
[INFO] 10.244.0.68:42669 - 35247 "AAAA IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000107164s
[INFO] 10.244.0.68:54082 - 25290 "A IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000113747s
[INFO] 10.244.0.68:49353 - 16403 "AAAA IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 29 0.785269024s
[INFO] 10.244.0.68:50305 - 22251 "A IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 56 0.785705761s
[INFO] 10.244.0.68:44848 - 40455 "AAAA IN grafana.com.reactory.svc.cluster.local. udp 67 false 1232" NXDOMAIN qr,aa,rd 149 0.000873133s
[INFO] 10.244.0.68:42893 - 60867 "A IN grafana.com.reactory.svc.cluster.local. udp 67 false 1232" NXDOMAIN qr,aa,rd 149 0.001458847s
[INFO] 10.244.0.68:44275 - 3775 "AAAA IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000136585s
[INFO] 10.244.0.68:47912 - 62048 "A IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000079876s
[INFO] 10.244.0.68:46674 - 12670 "AAAA IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000096376s
[INFO] 10.244.0.68:56258 - 14578 "A IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000078793s
[INFO] 10.244.0.68:38862 - 44897 "A IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 56 0.032832736s
[INFO] 10.244.0.68:49484 - 25438 "AAAA IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 29 0.033464326s
[INFO] 10.244.0.68:57789 - 27574 "A IN grafana.com.reactory.svc.cluster.local. udp 67 false 1232" NXDOMAIN qr,aa,rd 149 0.000545499s
[INFO] 10.244.0.68:55515 - 4597 "AAAA IN grafana.com.reactory.svc.cluster.local. udp 67 false 1232" NXDOMAIN qr,aa,rd 149 0.00078079s
[INFO] 10.244.0.68:52070 - 25028 "AAAA IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000262875s
[INFO] 10.244.0.68:59594 - 11219 "A IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000236208s
[INFO] 10.244.0.68:60496 - 21529 "AAAA IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000218666s
[INFO] 10.244.0.68:37259 - 6651 "A IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000181833s
[INFO] 10.244.0.68:58651 - 38734 "A IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 56 0.033866945s
[INFO] 10.244.0.68:56292 - 54518 "AAAA IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 29 0.034307195s
[INFO] 10.244.0.68:43171 - 10749 "A IN grafana.com.reactory.svc.cluster.local. udp 67 false 1232" NXDOMAIN qr,aa,rd 149 0.00152393s
[INFO] 10.244.0.68:38239 - 29188 "AAAA IN grafana.com.reactory.svc.cluster.local. udp 67 false 1232" NXDOMAIN qr,aa,rd 149 0.002188251s
[INFO] 10.244.0.68:33522 - 44377 "A IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000217329s
[INFO] 10.244.0.68:35896 - 18135 "AAAA IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000106998s
[INFO] 10.244.0.68:34259 - 5267 "AAAA IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000115081s
[INFO] 10.244.0.68:51144 - 27428 "A IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000094457s
[INFO] 10.244.0.68:40311 - 49430 "A IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 56 0.236427787s
[INFO] 10.244.0.68:49449 - 47999 "AAAA IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 29 0.23716669s
[INFO] 10.244.0.74:38634 - 59710 "AAAA IN telemetry.meilisearch.com.reactory.svc.cluster.local. udp 70 false 512" NXDOMAIN qr,aa,rd 163 0.000355163s
[INFO] 10.244.0.74:38634 - 59418 "A IN telemetry.meilisearch.com.reactory.svc.cluster.local. udp 70 false 512" NXDOMAIN qr,aa,rd 163 0.000455954s
[INFO] 10.244.0.74:41226 - 56729 "AAAA IN telemetry.meilisearch.com.svc.cluster.local. udp 61 false 512" NXDOMAIN qr,aa,rd 154 0.000113041s
[INFO] 10.244.0.74:41226 - 56395 "A IN telemetry.meilisearch.com.svc.cluster.local. udp 61 false 512" NXDOMAIN qr,aa,rd 154 0.000116249s
[INFO] 10.244.0.74:49376 - 14241 "AAAA IN telemetry.meilisearch.com.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.000124749s
[INFO] 10.244.0.74:49376 - 13950 "A IN telemetry.meilisearch.com.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.000070583s
[INFO] 10.244.0.74:47937 - 28741 "AAAA IN telemetry.meilisearch.com. udp 43 false 512" NOERROR qr,rd,ra 43 0.419285372s
[INFO] 10.244.0.74:47937 - 28533 "A IN telemetry.meilisearch.com. udp 43 false 512" NOERROR qr,rd,ra 125 0.419340246s
[INFO] 10.244.0.68:51848 - 27970 "A IN grafana.com.reactory.svc.cluster.local. udp 67 false 1232" NXDOMAIN qr,aa,rd 149 0.000792092s
[INFO] 10.244.0.68:53914 - 51740 "AAAA IN grafana.com.reactory.svc.cluster.local. udp 67 false 1232" NXDOMAIN qr,aa,rd 149 0.000986386s
[INFO] 10.244.0.68:53745 - 11025 "AAAA IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000134293s
[INFO] 10.244.0.68:40580 - 20422 "A IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.00008675s
[INFO] 10.244.0.68:33632 - 62906 "AAAA IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000161627s
[INFO] 10.244.0.68:41406 - 31235 "A IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.00011671s
[INFO] 10.244.0.68:57069 - 55766 "AAAA IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 29 0.031063992s
[INFO] 10.244.0.68:55608 - 60317 "A IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 56 0.031121826s


==> coredns [dcb5e6aade36] <==
[INFO] 10.244.0.93:59725 - 43456 "AAAA IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000122625s
[INFO] 10.244.0.93:45469 - 28106 "A IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.00016146s
[INFO] 10.244.0.93:34833 - 40672 "AAAA IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 29 0.033783354s
[INFO] 10.244.0.93:50232 - 23740 "A IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 56 0.034371858s
[INFO] 10.244.0.93:58110 - 58931 "A IN grafana.com.reactory.svc.cluster.local. udp 67 false 1232" NXDOMAIN qr,aa,rd 149 0.001102047s
[INFO] 10.244.0.93:50375 - 30256 "AAAA IN grafana.com.reactory.svc.cluster.local. udp 67 false 1232" NXDOMAIN qr,aa,rd 149 0.001648674s
[INFO] 10.244.0.93:60691 - 39065 "AAAA IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000199959s
[INFO] 10.244.0.93:34222 - 57837 "A IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000352293s
[INFO] 10.244.0.93:49040 - 30396 "AAAA IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000101667s
[INFO] 10.244.0.93:32944 - 2683 "A IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000209584s
[INFO] 10.244.0.93:51881 - 35668 "AAAA IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 29 0.034498575s
[INFO] 10.244.0.93:56864 - 62144 "A IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 56 0.034660408s
[INFO] 10.244.0.93:35741 - 54448 "AAAA IN grafana.com.reactory.svc.cluster.local. udp 67 false 1232" NXDOMAIN qr,aa,rd 149 0.000846673s
[INFO] 10.244.0.93:36177 - 27183 "A IN grafana.com.reactory.svc.cluster.local. udp 67 false 1232" NXDOMAIN qr,aa,rd 149 0.001015882s
[INFO] 10.244.0.93:47865 - 9462 "A IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000132043s
[INFO] 10.244.0.93:36711 - 11249 "AAAA IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000315086s
[INFO] 10.244.0.93:48763 - 427 "A IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000182876s
[INFO] 10.244.0.93:42474 - 55765 "AAAA IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000137792s
[INFO] 10.244.0.93:55562 - 20798 "A IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 56 0.029952128s
[INFO] 10.244.0.93:50189 - 59825 "AAAA IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 29 0.031497055s
[INFO] 10.244.0.93:34754 - 25345 "A IN grafana.com.reactory.svc.cluster.local. udp 67 false 1232" NXDOMAIN qr,aa,rd 149 0.00172217s
[INFO] 10.244.0.93:35556 - 11008 "AAAA IN grafana.com.reactory.svc.cluster.local. udp 67 false 1232" NXDOMAIN qr,aa,rd 149 0.002172045s
[INFO] 10.244.0.93:37744 - 11201 "A IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000195251s
[INFO] 10.244.0.93:34010 - 45706 "AAAA IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000189917s
[INFO] 10.244.0.93:36198 - 63591 "A IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000107959s
[INFO] 10.244.0.93:55495 - 6765 "AAAA IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000316501s
[INFO] 10.244.0.93:56022 - 45385 "A IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 56 0.036949697s
[INFO] 10.244.0.93:43845 - 59045 "AAAA IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 29 0.037059947s
[INFO] 10.244.0.93:40465 - 22295 "AAAA IN grafana.com.reactory.svc.cluster.local. udp 67 false 1232" NXDOMAIN qr,aa,rd 149 0.001312761s
[INFO] 10.244.0.93:44902 - 60123 "A IN grafana.com.reactory.svc.cluster.local. udp 67 false 1232" NXDOMAIN qr,aa,rd 149 0.001421304s
[INFO] 10.244.0.93:52988 - 33765 "AAAA IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000183751s
[INFO] 10.244.0.93:49495 - 18439 "A IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000237127s
[INFO] 10.244.0.93:58885 - 13149 "A IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000143792s
[INFO] 10.244.0.93:37300 - 21365 "AAAA IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000355961s
[INFO] 10.244.0.93:57148 - 49437 "A IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 56 0.03382011s
[INFO] 10.244.0.93:55926 - 34809 "AAAA IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 29 0.034398281s
[INFO] 10.244.0.93:41510 - 27231 "AAAA IN grafana.com.reactory.svc.cluster.local. udp 67 false 1232" NXDOMAIN qr,aa,rd 149 0.00072863s
[INFO] 10.244.0.93:59471 - 37518 "A IN grafana.com.reactory.svc.cluster.local. udp 67 false 1232" NXDOMAIN qr,aa,rd 149 0.000094375s
[INFO] 10.244.0.93:46632 - 49710 "AAAA IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000088918s
[INFO] 10.244.0.93:56359 - 56359 "A IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000198877s
[INFO] 10.244.0.93:35574 - 49666 "AAAA IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000082959s
[INFO] 10.244.0.93:32967 - 46914 "A IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000263169s
[INFO] 10.244.0.93:53243 - 28078 "AAAA IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 29 0.030713181s
[INFO] 10.244.0.93:54721 - 16745 "A IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 56 0.030066593s
[INFO] 10.244.0.102:37758 - 42892 "AAAA IN telemetry.meilisearch.com.reactory.svc.cluster.local. udp 70 false 512" NXDOMAIN qr,aa,rd 163 0.000337586s
[INFO] 10.244.0.102:37758 - 42642 "A IN telemetry.meilisearch.com.reactory.svc.cluster.local. udp 70 false 512" NXDOMAIN qr,aa,rd 163 0.000430837s
[INFO] 10.244.0.102:59583 - 40999 "A IN telemetry.meilisearch.com.svc.cluster.local. udp 61 false 512" NXDOMAIN qr,aa,rd 154 0.000080334s
[INFO] 10.244.0.102:59583 - 41463 "AAAA IN telemetry.meilisearch.com.svc.cluster.local. udp 61 false 512" NXDOMAIN qr,aa,rd 154 0.000184377s
[INFO] 10.244.0.102:51236 - 50480 "AAAA IN telemetry.meilisearch.com.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.000244044s
[INFO] 10.244.0.102:51236 - 50271 "A IN telemetry.meilisearch.com.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.000082709s
[INFO] 10.244.0.102:50407 - 6745 "A IN telemetry.meilisearch.com. udp 43 false 512" NOERROR qr,rd,ra 125 0.03510613s
[INFO] 10.244.0.102:50407 - 7126 "AAAA IN telemetry.meilisearch.com. udp 43 false 512" NOERROR qr,rd,ra 43 0.035857969s
[INFO] 10.244.0.93:53954 - 15695 "AAAA IN grafana.com.reactory.svc.cluster.local. udp 67 false 1232" NXDOMAIN qr,aa,rd 149 0.000859297s
[INFO] 10.244.0.93:34188 - 63227 "AAAA IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000214835s
[INFO] 10.244.0.93:53769 - 53525 "A IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000137168s
[INFO] 10.244.0.93:59450 - 31186 "A IN grafana.com.reactory.svc.cluster.local. udp 67 false 1232" NXDOMAIN qr,aa,rd 149 0.001911387s
[INFO] 10.244.0.93:42696 - 27820 "A IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000132751s
[INFO] 10.244.0.93:37627 - 25511 "AAAA IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000074459s
[INFO] 10.244.0.93:43063 - 50724 "A IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 56 0.033256578s
[INFO] 10.244.0.93:37830 - 10466 "AAAA IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 29 0.033347037s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=arm64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=arm64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=210b148df93a80eb872ecbeb7e35281b3c582c61
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_12_27T18_49_30_0700
                    minikube.k8s.io/version=v1.34.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 27 Dec 2024 16:49:28 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sat, 11 Jan 2025 00:20:59 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sat, 11 Jan 2025 00:18:16 +0000   Fri, 10 Jan 2025 01:35:09 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sat, 11 Jan 2025 00:18:16 +0000   Fri, 10 Jan 2025 01:35:09 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sat, 11 Jan 2025 00:18:16 +0000   Fri, 10 Jan 2025 01:35:09 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sat, 11 Jan 2025 00:18:16 +0000   Fri, 10 Jan 2025 02:52:55 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  10.211.55.4
  Hostname:    minikube
Capacity:
  cpu:                2
  ephemeral-storage:  17734596Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             5909564Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  17734596Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             5909564Ki
  pods:               110
System Info:
  Machine ID:                 1babe81b20ea4771bd4f89b75f12a37e
  System UUID:                4e4655de-a776-493e-ae71-6b196d10f349
  Boot ID:                    7af3c019-5501-4cbf-ac66-4f20bb92566b
  Kernel Version:             5.10.207
  OS Image:                   Buildroot 2023.02.9
  Operating System:           linux
  Architecture:               arm64
  Container Runtime Version:  docker://27.2.0
  Kubelet Version:            v1.31.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (23 in total)
  Namespace                   Name                                        CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                        ------------  ----------  ---------------  -------------  ---
  default                     reactory-nginx-66f8c7d4c5-9hlvd             0 (0%)        0 (0%)      0 (0%)           0 (0%)         9d
  ingress-nginx               ingress-nginx-admission-create-6d464        0 (0%)        0 (0%)      0 (0%)           0 (0%)         9d
  ingress-nginx               ingress-nginx-admission-patch-c2zbw         0 (0%)        0 (0%)      0 (0%)           0 (0%)         9d
  ingress-nginx               ingress-nginx-controller-bc57996ff-m24pp    100m (5%)     0 (0%)      90Mi (1%)        0 (0%)         9d
  kube-system                 coredns-6f6b679f8f-gdpck                    100m (5%)     0 (0%)      70Mi (1%)        170Mi (2%)     14d
  kube-system                 etcd-minikube                               100m (5%)     0 (0%)      100Mi (1%)       0 (0%)         14d
  kube-system                 kube-apiserver-minikube                     250m (12%)    0 (0%)      0 (0%)           0 (0%)         14d
  kube-system                 kube-controller-manager-minikube            200m (10%)    0 (0%)      0 (0%)           0 (0%)         14d
  kube-system                 kube-proxy-v8z2d                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         14d
  kube-system                 kube-scheduler-minikube                     100m (5%)     0 (0%)      0 (0%)           0 (0%)         14d
  kube-system                 storage-provisioner                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         14d
  reactory                    reactory-express-server-5d6f4cdd55-zxczj    0 (0%)        0 (0%)      0 (0%)           0 (0%)         8h
  reactory                    reactory-grafana-8f6f95f7c-9vtb8            0 (0%)        0 (0%)      0 (0%)           0 (0%)         46h
  reactory                    reactory-jaeger-7bd97f4cfc-894dg            0 (0%)        0 (0%)      0 (0%)           0 (0%)         2d20h
  reactory                    reactory-meilisearch-577c78589d-dczt5       0 (0%)        0 (0%)      0 (0%)           0 (0%)         2d
  reactory                    reactory-mongodb-5bf9f5d545-hnh7z           0 (0%)        0 (0%)      0 (0%)           0 (0%)         2d
  reactory                    reactory-nginx-6ddc48c85b-c2x8s             0 (0%)        0 (0%)      0 (0%)           0 (0%)         2d20h
  reactory                    reactory-nginx-6ddc48c85b-kmmgz             0 (0%)        0 (0%)      0 (0%)           0 (0%)         2d20h
  reactory                    reactory-postgres-5d56c6d77d-dq7v7          0 (0%)        0 (0%)      0 (0%)           0 (0%)         2d
  reactory                    reactory-prometheus-57dcc99f-dnd5r          0 (0%)        0 (0%)      0 (0%)           0 (0%)         9h
  reactory                    reactory-pwa-client-5bf5bdf4bf-5kvqj        0 (0%)        0 (0%)      0 (0%)           0 (0%)         2d20h
  reactory                    reactory-pwa-client-5bf5bdf4bf-t7hp9        0 (0%)        0 (0%)      0 (0%)           0 (0%)         2d20h
  reactory                    reactory-redis-6658b5b567-xxl6s             0 (0%)        0 (0%)      0 (0%)           0 (0%)         9h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (42%)  0 (0%)
  memory             260Mi (4%)  170Mi (2%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
  hugepages-32Mi     0 (0%)      0 (0%)
  hugepages-64Ki     0 (0%)      0 (0%)
Events:              <none>


==> dmesg <==
[Jan10 19:01] ACPI: SRAT not present
[  +0.000000] ACPI GTDT: Revision:1 doesn't support Platform Timers.
[  +0.000000] ACPI PPTT: No PPTT table found, CPU and cache topology may be inaccurate
[  +0.012007] KASLR disabled due to lack of seed
[  +0.004767] acpi PNP0A08:00: [Firmware Bug]: ECAM area [mem 0x02300000-0x023fffff] not reserved in ACPI namespace
[  +0.033416] No ACPI PMU IRQ for CPU0
[  +0.000096] No ACPI PMU IRQ for CPU1
[  +0.689917] EINJ: EINJ table not found.
[  +0.000110] ACPI GTDT: Revision:1 doesn't support Platform Timers.
[  +0.002827] cacheinfo: Unable to detect cache hierarchy for CPU 0
[  +0.001078] ahci PRL4010:00: supply ahci not found, using dummy regulator
[  +0.000190] ahci PRL4010:00: supply phy not found, using dummy regulator
[  +0.000188] ahci PRL4010:00: supply target not found, using dummy regulator
[  +0.007299] ehci-pci 0000:00:02.0: Enabling legacy PCI PM
[  +2.433107] systemd-fstab-generator[137]: Ignoring "noauto" option for root device
[  +0.141065] platform regulatory.0: Direct firmware load for regulatory.db failed with error -2
[  +0.000230] platform regulatory.0: Falling back to sysfs fallback for: regulatory.db
[ +12.273645] systemd-fstab-generator[515]: Ignoring "noauto" option for root device
[  +0.084090] systemd-fstab-generator[527]: Ignoring "noauto" option for root device
[  +3.409673] systemd-fstab-generator[835]: Ignoring "noauto" option for root device
[  +0.187092] systemd-fstab-generator[873]: Ignoring "noauto" option for root device
[  +0.070688] systemd-fstab-generator[885]: Ignoring "noauto" option for root device
[  +0.083403] systemd-fstab-generator[899]: Ignoring "noauto" option for root device
[  +2.238416] kauditd_printk_skb: 197 callbacks suppressed
[  +0.110241] systemd-fstab-generator[1115]: Ignoring "noauto" option for root device
[  +0.078690] systemd-fstab-generator[1127]: Ignoring "noauto" option for root device
[  +0.067450] systemd-fstab-generator[1139]: Ignoring "noauto" option for root device
[  +0.080001] systemd-fstab-generator[1154]: Ignoring "noauto" option for root device
[  +0.485093] systemd-fstab-generator[1278]: Ignoring "noauto" option for root device
[  +0.995462] systemd-fstab-generator[1400]: Ignoring "noauto" option for root device
[  +4.476897] kauditd_printk_skb: 246 callbacks suppressed
[  +0.977698] systemd-fstab-generator[2130]: Ignoring "noauto" option for root device
[  +7.732418] kauditd_printk_skb: 66 callbacks suppressed
[Jan10 19:02] kauditd_printk_skb: 219 callbacks suppressed
[  +5.943178] kauditd_printk_skb: 3 callbacks suppressed
[  +5.384994] kauditd_printk_skb: 2 callbacks suppressed
[  +5.773136] kauditd_printk_skb: 2 callbacks suppressed
[Jan10 20:42] kauditd_printk_skb: 1 callbacks suppressed
[Jan10 21:08] kauditd_printk_skb: 27 callbacks suppressed
[Jan10 21:19] kauditd_printk_skb: 40 callbacks suppressed
[Jan10 22:00] kauditd_printk_skb: 1 callbacks suppressed


==> etcd [873cff1dde2a] <==
{"level":"info","ts":"2025-01-10T22:02:59.517873Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":290258,"took":"4.464074ms","hash":2716111576,"current-db-size-bytes":5124096,"current-db-size":"5.1 MB","current-db-size-in-use-bytes":1863680,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-01-10T22:02:59.517953Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2716111576,"revision":290258,"compact-revision":290035}
{"level":"info","ts":"2025-01-10T22:51:43.791443Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":290452}
{"level":"info","ts":"2025-01-10T22:51:43.796716Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":290452,"took":"3.696333ms","hash":3753051657,"current-db-size-bytes":5124096,"current-db-size":"5.1 MB","current-db-size-in-use-bytes":2007040,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2025-01-10T22:51:43.796786Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3753051657,"revision":290452,"compact-revision":290258}
{"level":"info","ts":"2025-01-10T22:56:43.798738Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":290673}
{"level":"info","ts":"2025-01-10T22:56:43.804603Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":290673,"took":"4.771999ms","hash":2516934808,"current-db-size-bytes":5124096,"current-db-size":"5.1 MB","current-db-size-in-use-bytes":1961984,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2025-01-10T22:56:43.804715Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2516934808,"revision":290673,"compact-revision":290452}
{"level":"info","ts":"2025-01-10T23:01:43.806772Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":290921}
{"level":"info","ts":"2025-01-10T23:01:43.812869Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":290921,"took":"5.076899ms","hash":3633442170,"current-db-size-bytes":5124096,"current-db-size":"5.1 MB","current-db-size-in-use-bytes":1945600,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-01-10T23:01:43.812927Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3633442170,"revision":290921,"compact-revision":290673}
{"level":"info","ts":"2025-01-10T23:06:43.811667Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":291168}
{"level":"info","ts":"2025-01-10T23:06:43.814538Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":291168,"took":"2.096244ms","hash":2098372385,"current-db-size-bytes":5124096,"current-db-size":"5.1 MB","current-db-size-in-use-bytes":1908736,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-01-10T23:06:43.814581Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2098372385,"revision":291168,"compact-revision":290921}
{"level":"info","ts":"2025-01-10T23:11:43.817498Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":291416}
{"level":"info","ts":"2025-01-10T23:11:43.821168Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":291416,"took":"2.777015ms","hash":3180364362,"current-db-size-bytes":5124096,"current-db-size":"5.1 MB","current-db-size-in-use-bytes":1912832,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-01-10T23:11:43.821233Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3180364362,"revision":291416,"compact-revision":291168}
{"level":"info","ts":"2025-01-10T23:16:43.831510Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":291663}
{"level":"info","ts":"2025-01-10T23:16:43.835698Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":291663,"took":"3.385901ms","hash":1632063728,"current-db-size-bytes":5124096,"current-db-size":"5.1 MB","current-db-size-in-use-bytes":1945600,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-01-10T23:16:43.835756Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1632063728,"revision":291663,"compact-revision":291416}
{"level":"info","ts":"2025-01-10T23:21:43.838036Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":291911}
{"level":"info","ts":"2025-01-10T23:21:43.840916Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":291911,"took":"2.500927ms","hash":2600786997,"current-db-size-bytes":5124096,"current-db-size":"5.1 MB","current-db-size-in-use-bytes":1937408,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-01-10T23:21:43.840972Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2600786997,"revision":291911,"compact-revision":291663}
{"level":"info","ts":"2025-01-10T23:26:43.848873Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":292159}
{"level":"info","ts":"2025-01-10T23:26:43.854730Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":292159,"took":"4.407943ms","hash":1605874143,"current-db-size-bytes":5124096,"current-db-size":"5.1 MB","current-db-size-in-use-bytes":1921024,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-01-10T23:26:43.854806Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1605874143,"revision":292159,"compact-revision":291911}
{"level":"info","ts":"2025-01-10T23:31:43.857150Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":292407}
{"level":"info","ts":"2025-01-10T23:31:43.862663Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":292407,"took":"4.429906ms","hash":3718535768,"current-db-size-bytes":5124096,"current-db-size":"5.1 MB","current-db-size-in-use-bytes":1925120,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-01-10T23:31:43.862745Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3718535768,"revision":292407,"compact-revision":292159}
{"level":"info","ts":"2025-01-10T23:36:43.867080Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":292655}
{"level":"info","ts":"2025-01-10T23:36:43.874437Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":292655,"took":"6.055903ms","hash":2690066879,"current-db-size-bytes":5124096,"current-db-size":"5.1 MB","current-db-size-in-use-bytes":1933312,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-01-10T23:36:43.874523Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2690066879,"revision":292655,"compact-revision":292407}
{"level":"info","ts":"2025-01-10T23:41:43.874399Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":292902}
{"level":"info","ts":"2025-01-10T23:41:43.880282Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":292902,"took":"4.598216ms","hash":2210207851,"current-db-size-bytes":5124096,"current-db-size":"5.1 MB","current-db-size-in-use-bytes":1904640,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-01-10T23:41:43.880358Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2210207851,"revision":292902,"compact-revision":292655}
{"level":"info","ts":"2025-01-10T23:46:43.881990Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":293152}
{"level":"info","ts":"2025-01-10T23:46:43.888588Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":293152,"took":"5.756856ms","hash":4091416449,"current-db-size-bytes":5124096,"current-db-size":"5.1 MB","current-db-size-in-use-bytes":1880064,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-01-10T23:46:43.888664Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4091416449,"revision":293152,"compact-revision":292902}
{"level":"info","ts":"2025-01-10T23:51:43.893128Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":293404}
{"level":"info","ts":"2025-01-10T23:51:43.898908Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":293404,"took":"4.478246ms","hash":1901275029,"current-db-size-bytes":5124096,"current-db-size":"5.1 MB","current-db-size-in-use-bytes":1871872,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-01-10T23:51:43.899004Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1901275029,"revision":293404,"compact-revision":293152}
{"level":"info","ts":"2025-01-10T23:56:43.903894Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":293654}
{"level":"info","ts":"2025-01-10T23:56:43.909042Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":293654,"took":"3.595569ms","hash":967248086,"current-db-size-bytes":5124096,"current-db-size":"5.1 MB","current-db-size-in-use-bytes":1855488,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-01-10T23:56:43.909104Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":967248086,"revision":293654,"compact-revision":293404}
{"level":"info","ts":"2025-01-11T00:01:43.912111Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":293903}
{"level":"info","ts":"2025-01-11T00:01:43.916753Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":293903,"took":"3.769361ms","hash":48877281,"current-db-size-bytes":5124096,"current-db-size":"5.1 MB","current-db-size-in-use-bytes":1814528,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-01-11T00:01:43.916838Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":48877281,"revision":293903,"compact-revision":293654}
{"level":"info","ts":"2025-01-11T00:06:43.920490Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":294150}
{"level":"info","ts":"2025-01-11T00:06:43.925769Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":294150,"took":"4.257357ms","hash":899744775,"current-db-size-bytes":5124096,"current-db-size":"5.1 MB","current-db-size-in-use-bytes":1855488,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-01-11T00:06:43.925859Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":899744775,"revision":294150,"compact-revision":293903}
{"level":"info","ts":"2025-01-11T00:11:43.928644Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":294398}
{"level":"info","ts":"2025-01-11T00:11:43.934182Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":294398,"took":"4.278067ms","hash":819635035,"current-db-size-bytes":5124096,"current-db-size":"5.1 MB","current-db-size-in-use-bytes":1867776,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-01-11T00:11:43.934288Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":819635035,"revision":294398,"compact-revision":294150}
{"level":"info","ts":"2025-01-11T00:16:43.949281Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":294646}
{"level":"info","ts":"2025-01-11T00:16:43.958681Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":294646,"took":"9.047141ms","hash":4245569306,"current-db-size-bytes":5124096,"current-db-size":"5.1 MB","current-db-size-in-use-bytes":1863680,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-01-11T00:16:43.958782Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4245569306,"revision":294646,"compact-revision":294398}
{"level":"info","ts":"2025-01-11T00:18:16.527003Z","caller":"etcdserver/server.go:1451","msg":"triggering snapshot","local-member-id":"78a30be28647bb39","local-member-applied-index":370037,"local-member-snapshot-index":360036,"local-member-snapshot-count":10000}
{"level":"info","ts":"2025-01-11T00:18:16.534307Z","caller":"etcdserver/server.go:2471","msg":"saved snapshot","snapshot-index":370037}
{"level":"info","ts":"2025-01-11T00:18:16.534488Z","caller":"etcdserver/server.go:2501","msg":"compacted Raft logs","compact-index":365037}
{"level":"info","ts":"2025-01-11T00:18:43.500886Z","caller":"fileutil/purge.go:96","msg":"purged","path":"/var/lib/minikube/etcd/member/snap/0000000000000005-000000000004e220.snap"}


==> etcd [b16f2bdb58f2] <==
{"level":"info","ts":"2025-01-10T01:35:08.371718Z","caller":"embed/etcd.go:599","msg":"serving peer traffic","address":"10.211.55.4:2380"}
{"level":"info","ts":"2025-01-10T01:35:08.371725Z","caller":"embed/etcd.go:571","msg":"cmux::serve","address":"10.211.55.4:2380"}
{"level":"info","ts":"2025-01-10T01:35:08.967459Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"78a30be28647bb39 is starting a new election at term 5"}
{"level":"info","ts":"2025-01-10T01:35:08.967599Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"78a30be28647bb39 became pre-candidate at term 5"}
{"level":"info","ts":"2025-01-10T01:35:08.968015Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"78a30be28647bb39 received MsgPreVoteResp from 78a30be28647bb39 at term 5"}
{"level":"info","ts":"2025-01-10T01:35:08.968052Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"78a30be28647bb39 became candidate at term 6"}
{"level":"info","ts":"2025-01-10T01:35:08.968067Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"78a30be28647bb39 received MsgVoteResp from 78a30be28647bb39 at term 6"}
{"level":"info","ts":"2025-01-10T01:35:08.968099Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"78a30be28647bb39 became leader at term 6"}
{"level":"info","ts":"2025-01-10T01:35:08.968122Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: 78a30be28647bb39 elected leader 78a30be28647bb39 at term 6"}
{"level":"info","ts":"2025-01-10T01:35:08.972823Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-01-10T01:35:08.973179Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-01-10T01:35:08.974099Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-01-10T01:35:08.974135Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-01-10T01:35:08.975710Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-01-10T01:35:08.977369Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"10.211.55.4:2379"}
{"level":"info","ts":"2025-01-10T01:35:08.972835Z","caller":"etcdserver/server.go:2118","msg":"published local member to cluster through raft","local-member-id":"78a30be28647bb39","local-member-attributes":"{Name:minikube ClientURLs:[https://10.211.55.4:2379]}","request-path":"/0/members/78a30be28647bb39/attributes","cluster-id":"a9e0a92479ee4333","publish-timeout":"7s"}
{"level":"info","ts":"2025-01-10T01:35:08.979936Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-01-10T01:35:08.981320Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-01-10T01:45:09.014382Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":274685}
{"level":"info","ts":"2025-01-10T01:45:09.036476Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":274685,"took":"21.16656ms","hash":260516383,"current-db-size-bytes":4489216,"current-db-size":"4.5 MB","current-db-size-in-use-bytes":2007040,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2025-01-10T01:45:09.036591Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":260516383,"revision":274685,"compact-revision":273863}
{"level":"info","ts":"2025-01-10T01:50:09.020795Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":274941}
{"level":"info","ts":"2025-01-10T01:50:09.025230Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":274941,"took":"3.274031ms","hash":1060351483,"current-db-size-bytes":4489216,"current-db-size":"4.5 MB","current-db-size-in-use-bytes":2142208,"current-db-size-in-use":"2.1 MB"}
{"level":"info","ts":"2025-01-10T01:50:09.025318Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1060351483,"revision":274941,"compact-revision":274685}
{"level":"info","ts":"2025-01-10T01:55:09.029143Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":275195}
{"level":"info","ts":"2025-01-10T01:55:09.034829Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":275195,"took":"4.146967ms","hash":3167771915,"current-db-size-bytes":4489216,"current-db-size":"4.5 MB","current-db-size-in-use-bytes":2064384,"current-db-size-in-use":"2.1 MB"}
{"level":"info","ts":"2025-01-10T01:55:09.034947Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3167771915,"revision":275195,"compact-revision":274941}
{"level":"info","ts":"2025-01-10T02:00:09.034346Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":275446}
{"level":"info","ts":"2025-01-10T02:00:09.038145Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":275446,"took":"2.885963ms","hash":2326083942,"current-db-size-bytes":4489216,"current-db-size":"4.5 MB","current-db-size-in-use-bytes":2060288,"current-db-size-in-use":"2.1 MB"}
{"level":"info","ts":"2025-01-10T02:00:09.038218Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2326083942,"revision":275446,"compact-revision":275195}
{"level":"info","ts":"2025-01-10T02:05:09.050073Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":275699}
{"level":"info","ts":"2025-01-10T02:05:09.058113Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":275699,"took":"6.276566ms","hash":492765302,"current-db-size-bytes":4489216,"current-db-size":"4.5 MB","current-db-size-in-use-bytes":2097152,"current-db-size-in-use":"2.1 MB"}
{"level":"info","ts":"2025-01-10T02:05:09.058165Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":492765302,"revision":275699,"compact-revision":275446}
{"level":"info","ts":"2025-01-10T02:10:09.065291Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":275952}
{"level":"info","ts":"2025-01-10T02:10:09.072184Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":275952,"took":"5.712114ms","hash":3970786932,"current-db-size-bytes":4489216,"current-db-size":"4.5 MB","current-db-size-in-use-bytes":2592768,"current-db-size-in-use":"2.6 MB"}
{"level":"info","ts":"2025-01-10T02:10:09.072309Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3970786932,"revision":275952,"compact-revision":275699}
{"level":"info","ts":"2025-01-10T02:15:09.069355Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":276314}
{"level":"info","ts":"2025-01-10T02:15:09.074858Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":276314,"took":"4.661336ms","hash":2907490581,"current-db-size-bytes":4489216,"current-db-size":"4.5 MB","current-db-size-in-use-bytes":2801664,"current-db-size-in-use":"2.8 MB"}
{"level":"info","ts":"2025-01-10T02:15:09.074967Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2907490581,"revision":276314,"compact-revision":275952}
{"level":"info","ts":"2025-01-10T02:20:09.077560Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":276571}
{"level":"info","ts":"2025-01-10T02:20:09.080966Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":276571,"took":"2.857676ms","hash":2302102057,"current-db-size-bytes":4489216,"current-db-size":"4.5 MB","current-db-size-in-use-bytes":2134016,"current-db-size-in-use":"2.1 MB"}
{"level":"info","ts":"2025-01-10T02:20:09.081027Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2302102057,"revision":276571,"compact-revision":276314}
{"level":"info","ts":"2025-01-10T02:25:09.082929Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":276826}
{"level":"info","ts":"2025-01-10T02:25:09.089759Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":276826,"took":"5.55324ms","hash":2403238477,"current-db-size-bytes":4489216,"current-db-size":"4.5 MB","current-db-size-in-use-bytes":2084864,"current-db-size-in-use":"2.1 MB"}
{"level":"info","ts":"2025-01-10T02:25:09.089816Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2403238477,"revision":276826,"compact-revision":276571}
{"level":"info","ts":"2025-01-10T02:30:09.090928Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":277077}
{"level":"info","ts":"2025-01-10T02:30:09.097128Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":277077,"took":"4.534316ms","hash":1562573758,"current-db-size-bytes":4489216,"current-db-size":"4.5 MB","current-db-size-in-use-bytes":2633728,"current-db-size-in-use":"2.6 MB"}
{"level":"info","ts":"2025-01-10T02:30:09.097213Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1562573758,"revision":277077,"compact-revision":276826}
{"level":"info","ts":"2025-01-10T02:35:09.100676Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":277443}
{"level":"info","ts":"2025-01-10T02:35:09.110535Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":277443,"took":"9.044327ms","hash":802547167,"current-db-size-bytes":5124096,"current-db-size":"5.1 MB","current-db-size-in-use-bytes":3067904,"current-db-size-in-use":"3.1 MB"}
{"level":"info","ts":"2025-01-10T02:35:09.110609Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":802547167,"revision":277443,"compact-revision":277077}
{"level":"info","ts":"2025-01-10T02:40:09.104681Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":277698}
{"level":"info","ts":"2025-01-10T02:40:09.108702Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":277698,"took":"3.203374ms","hash":2232856726,"current-db-size-bytes":5124096,"current-db-size":"5.1 MB","current-db-size-in-use-bytes":2752512,"current-db-size-in-use":"2.8 MB"}
{"level":"info","ts":"2025-01-10T02:40:09.108757Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2232856726,"revision":277698,"compact-revision":277443}
{"level":"info","ts":"2025-01-10T02:45:09.110949Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":277961}
{"level":"info","ts":"2025-01-10T02:45:09.117791Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":277961,"took":"5.924228ms","hash":3208526729,"current-db-size-bytes":5124096,"current-db-size":"5.1 MB","current-db-size-in-use-bytes":2609152,"current-db-size-in-use":"2.6 MB"}
{"level":"info","ts":"2025-01-10T02:45:09.117894Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3208526729,"revision":277961,"compact-revision":277698}
{"level":"info","ts":"2025-01-10T02:50:09.123597Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":278213}
{"level":"info","ts":"2025-01-10T02:50:09.129693Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":278213,"took":"5.574999ms","hash":2594391735,"current-db-size-bytes":5124096,"current-db-size":"5.1 MB","current-db-size-in-use-bytes":1949696,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-01-10T02:50:09.129765Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2594391735,"revision":278213,"compact-revision":277961}


==> kernel <==
 00:21:08 up  5:19,  0 users,  load average: 0.31, 0.55, 0.50
Linux minikube 5.10.207 #1 SMP PREEMPT Tue Sep 3 18:23:52 UTC 2024 aarch64 GNU/Linux
PRETTY_NAME="Buildroot 2023.02.9"


==> kube-apiserver [38dadbda977e] <==
I0110 01:35:09.455503       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0110 01:35:09.455595       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I0110 01:35:09.455615       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I0110 01:35:09.455634       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I0110 01:35:09.455646       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0110 01:35:09.455687       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0110 01:35:09.455709       1 controller.go:78] Starting OpenAPI AggregationController
I0110 01:35:09.458157       1 local_available_controller.go:156] Starting LocalAvailability controller
I0110 01:35:09.458182       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I0110 01:35:09.458202       1 aggregator.go:169] waiting for initial CRD sync...
I0110 01:35:09.458254       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0110 01:35:09.458443       1 controller.go:119] Starting legacy_token_tracking_controller
I0110 01:35:09.458464       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0110 01:35:09.458495       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0110 01:35:09.458531       1 system_namespaces_controller.go:66] Starting system namespaces controller
I0110 01:35:09.458561       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0110 01:35:09.458591       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0110 01:35:09.464091       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0110 01:35:09.464125       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0110 01:35:09.477496       1 controller.go:142] Starting OpenAPI controller
I0110 01:35:09.477519       1 controller.go:90] Starting OpenAPI V3 controller
I0110 01:35:09.477526       1 naming_controller.go:294] Starting NamingConditionController
I0110 01:35:09.477544       1 establishing_controller.go:81] Starting EstablishingController
I0110 01:35:09.477567       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0110 01:35:09.477574       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0110 01:35:09.477578       1 crd_finalizer.go:269] Starting CRDFinalizer
I0110 01:35:09.553086       1 shared_informer.go:320] Caches are synced for node_authorizer
I0110 01:35:09.554484       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0110 01:35:09.554522       1 policy_source.go:224] refreshing policies
I0110 01:35:09.554562       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0110 01:35:09.554570       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0110 01:35:09.555968       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0110 01:35:09.556009       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0110 01:35:09.555971       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0110 01:35:09.558731       1 handler_discovery.go:450] Starting ResourceDiscoveryManager
I0110 01:35:09.558809       1 shared_informer.go:320] Caches are synced for configmaps
I0110 01:35:09.558731       1 cache.go:39] Caches are synced for LocalAvailability controller
E0110 01:35:09.560543       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I0110 01:35:09.564908       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0110 01:35:09.564959       1 aggregator.go:171] initial CRD sync complete...
I0110 01:35:09.564981       1 autoregister_controller.go:144] Starting autoregister controller
I0110 01:35:09.565003       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0110 01:35:09.565008       1 cache.go:39] Caches are synced for autoregister controller
I0110 01:35:09.588953       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0110 01:35:10.463022       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0110 01:35:11.117761       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0110 01:35:11.122188       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0110 01:35:11.133644       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I0110 01:35:11.141437       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0110 01:35:11.143520       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0110 01:35:13.284753       1 controller.go:615] quota admission added evaluator for: endpoints
I0110 01:35:13.333679       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
E0110 02:06:16.060847       1 conn.go:339] Error on socket receive: read tcp 10.211.55.4:8443->10.211.55.2:64471: use of closed network connection
E0110 02:06:27.053599       1 conn.go:339] Error on socket receive: read tcp 10.211.55.4:8443->10.211.55.2:64551: use of closed network connection
E0110 02:07:02.791360       1 conn.go:339] Error on socket receive: read tcp 10.211.55.4:8443->10.211.55.2:64773: use of closed network connection
I0110 02:08:04.016315       1 controller.go:615] quota admission added evaluator for: replicasets.apps
E0110 02:21:10.653590       1 conn.go:339] Error on socket receive: read tcp 10.211.55.4:8443->10.211.55.2:54167: use of closed network connection
E0110 02:26:54.815974       1 conn.go:339] Error on socket receive: read tcp 10.211.55.4:8443->10.211.55.2:56565: use of closed network connection
E0110 02:39:22.522588       1 conn.go:339] Error on socket receive: read tcp 10.211.55.4:8443->10.211.55.2:61643: use of closed network connection
E0110 02:49:05.341962       1 conn.go:339] Error on socket receive: read tcp 10.211.55.4:8443->10.211.55.2:49466: use of closed network connection


==> kube-apiserver [69e63a48845d] <==
E0110 20:44:31.286344       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E0110 20:44:31.286795       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E0110 20:44:31.287417       1 writers.go:135] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E0110 20:44:31.287995       1 writers.go:135] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E0110 20:44:31.288747       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="3.662193ms" method="PUT" path="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube" result=null
E0110 20:44:31.289444       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="4.128779ms" method="PUT" path="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube" result=null
E0110 20:44:49.125599       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, context canceled]"
E0110 20:44:49.126957       1 writers.go:122] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E0110 20:44:49.128957       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E0110 20:44:49.131024       1 writers.go:135] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E0110 20:44:49.133535       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="8.10735ms" method="GET" path="/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath" result=null
E0110 21:32:57.569073       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0110 21:32:57.569243       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0110 21:33:12.839518       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, context canceled]"
E0110 21:33:12.840157       1 writers.go:122] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E0110 21:33:12.841893       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E0110 21:33:12.847469       1 writers.go:135] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E0110 21:33:12.848961       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="9.623194ms" method="GET" path="/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath" result=null
E0110 21:33:12.858548       1 writers.go:122] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E0110 21:33:12.858674       1 finisher.go:175] "Unhandled Error" err="FinishRequest: post-timeout activity - time-elapsed: 8.792µs, panicked: false, err: context canceled, panic-reason: <nil>" logger="UnhandledError"
E0110 21:33:12.862542       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E0110 21:33:12.863919       1 writers.go:135] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E0110 21:33:12.865951       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="7.497304ms" method="POST" path="/api/v1/namespaces/default/events" result=null
E0110 22:12:37.440874       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, context canceled]"
E0110 22:12:37.441256       1 writers.go:122] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E0110 22:12:37.442339       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E0110 22:12:37.443950       1 writers.go:135] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E0110 22:12:37.445084       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="3.025605ms" method="GET" path="/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath" result=null
E0110 22:12:37.446832       1 finisher.go:175] "Unhandled Error" err="FinishRequest: post-timeout activity - time-elapsed: 5.167µs, panicked: false, err: context canceled, panic-reason: <nil>" logger="UnhandledError"
E0110 22:12:37.446860       1 writers.go:122] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E0110 22:12:37.449064       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E0110 22:12:37.450324       1 writers.go:135] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E0110 22:12:37.452005       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="6.634506ms" method="POST" path="/api/v1/namespaces/default/events" result=null
E0110 22:22:51.165722       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, context canceled]"
E0110 22:22:51.165939       1 writers.go:122] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E0110 22:22:51.167754       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E0110 22:22:51.168972       1 writers.go:135] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E0110 22:22:51.170562       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="4.653492ms" method="GET" path="/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath" result=null
E0110 22:31:24.051429       1 writers.go:122] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E0110 22:31:24.052183       1 finisher.go:175] "Unhandled Error" err="FinishRequest: post-timeout activity - time-elapsed: 670.671µs, panicked: false, err: context canceled, panic-reason: <nil>" logger="UnhandledError"
E0110 22:31:24.055056       1 writers.go:122] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E0110 22:31:24.056437       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E0110 22:31:24.056485       1 finisher.go:175] "Unhandled Error" err="FinishRequest: post-timeout activity - time-elapsed: 1.371927ms, panicked: false, err: context canceled, panic-reason: <nil>" logger="UnhandledError"
E0110 22:31:24.057240       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E0110 22:31:24.057675       1 writers.go:135] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E0110 22:31:24.060429       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="11.185372ms" method="PUT" path="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube" result=null
E0110 22:31:24.061375       1 writers.go:135] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E0110 22:31:24.062538       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="15.101483ms" method="PUT" path="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube" result=null
E0110 22:37:28.537296       1 writers.go:122] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E0110 22:37:28.538597       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E0110 22:37:28.539273       1 finisher.go:175] "Unhandled Error" err="FinishRequest: post-timeout activity - time-elapsed: 1.928639ms, panicked: false, err: context canceled, panic-reason: <nil>" logger="UnhandledError"
E0110 22:37:28.540049       1 writers.go:135] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E0110 22:37:28.541258       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="4.112946ms" method="PUT" path="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube" result=null
E0110 22:47:33.628223       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, context canceled]"
E0110 22:47:33.628283       1 writers.go:122] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E0110 22:47:33.629399       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E0110 22:47:33.631381       1 writers.go:135] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E0110 22:47:33.634375       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="6.145503ms" method="GET" path="/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath" result=null
E0110 22:48:26.828059       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0110 22:48:26.829037       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"


==> kube-controller-manager [17606f5b8c98] <==
I0110 23:19:38.152851       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0110 23:19:47.167485       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0110 23:22:11.497215       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0110 23:24:26.174838       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0110 23:24:38.171818       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0110 23:24:51.168371       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0110 23:25:04.152704       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0110 23:27:18.522958       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0110 23:29:43.173262       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0110 23:29:55.167233       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0110 23:30:01.162349       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0110 23:30:14.182590       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0110 23:32:24.872226       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0110 23:34:48.170144       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0110 23:35:00.162090       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0110 23:35:12.159040       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0110 23:35:23.162993       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0110 23:37:29.795903       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0110 23:39:52.154633       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0110 23:40:03.168163       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0110 23:40:20.175159       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0110 23:40:34.172724       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0110 23:42:37.007516       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0110 23:45:01.172304       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0110 23:45:15.178126       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0110 23:45:22.169770       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0110 23:45:36.164489       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0110 23:47:43.406186       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0110 23:50:18.168433       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0110 23:50:30.172772       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0110 23:50:32.162313       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0110 23:50:42.155961       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0110 23:52:49.136903       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0110 23:55:25.165015       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0110 23:55:36.160011       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0110 23:55:41.159667       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0110 23:55:52.161880       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0110 23:57:54.662132       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0111 00:00:31.158736       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0111 00:00:42.153105       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0111 00:00:51.158718       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0111 00:01:05.149261       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0111 00:03:00.979061       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0111 00:05:40.170328       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0111 00:05:52.179531       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0111 00:05:54.182270       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0111 00:06:05.166153       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0111 00:08:06.335357       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0111 00:10:40.166868       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0111 00:10:51.170538       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0111 00:11:00.162139       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0111 00:11:15.165202       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0111 00:13:11.684605       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0111 00:15:46.169608       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0111 00:15:57.170212       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0111 00:16:10.165426       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0111 00:16:25.166907       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0111 00:18:16.653055       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0111 00:20:53.164962       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0111 00:21:06.174056       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"


==> kube-controller-manager [2462b115d9cb] <==
I0110 02:24:21.550388       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0110 02:24:30.566897       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0110 02:24:38.429924       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="reactory/reactory-prometheus-c4797ff94" duration="39.958µs"
I0110 02:24:52.559486       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="reactory/reactory-prometheus-c4797ff94" duration="137.749µs"
I0110 02:25:23.087445       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="reactory/reactory-prometheus-c4797ff94" duration="15.709µs"
I0110 02:25:23.087980       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="reactory/reactory-express-server-866cbfcd6d" duration="5.083µs"
I0110 02:25:23.130779       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="reactory/reactory-prometheus" err="EndpointSlice informer cache is out of date"
I0110 02:25:23.175907       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="reactory/reactory-prometheus-7b8cfb77bd" duration="28.599844ms"
I0110 02:25:23.181679       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="reactory/reactory-express-server-866cbfcd6d" duration="34.165046ms"
I0110 02:25:23.192247       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="reactory/reactory-express-server-866cbfcd6d" duration="10.544405ms"
I0110 02:25:23.192278       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="reactory/reactory-express-server-866cbfcd6d" duration="20.417µs"
I0110 02:25:23.197662       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="reactory/reactory-prometheus-7b8cfb77bd" duration="21.728434ms"
I0110 02:25:23.204226       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="reactory/reactory-express-server-866cbfcd6d" duration="14.125µs"
I0110 02:25:23.216830       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="reactory/reactory-prometheus-7b8cfb77bd" duration="19.151979ms"
I0110 02:25:23.216876       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="reactory/reactory-prometheus-7b8cfb77bd" duration="32.291µs"
I0110 02:25:24.652357       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="reactory/reactory-express-server-866cbfcd6d" duration="27.541µs"
I0110 02:25:26.708017       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="reactory/reactory-prometheus-7b8cfb77bd" duration="41.416µs"
I0110 02:25:30.827188       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="reactory/reactory-prometheus-7b8cfb77bd" duration="37.374µs"
I0110 02:25:31.855031       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="reactory/reactory-prometheus-7b8cfb77bd" duration="30.041µs"
I0110 02:25:49.313386       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="reactory/reactory-prometheus-7b8cfb77bd" duration="67.458µs"
I0110 02:26:00.558210       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="reactory/reactory-prometheus-7b8cfb77bd" duration="606.12µs"
I0110 02:26:17.998037       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="reactory/reactory-prometheus-7b8cfb77bd" duration="62.332µs"
I0110 02:26:29.562579       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="reactory/reactory-prometheus-7b8cfb77bd" duration="172.332µs"
I0110 02:27:15.439461       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="reactory/reactory-prometheus-7b8cfb77bd" duration="39.333µs"
I0110 02:27:27.548877       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="reactory/reactory-prometheus-7b8cfb77bd" duration="59.917µs"
I0110 02:28:41.053135       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0110 02:28:41.432921       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="reactory/reactory-prometheus-7b8cfb77bd" duration="93.584µs"
I0110 02:28:55.562470       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="reactory/reactory-prometheus-7b8cfb77bd" duration="132.709µs"
I0110 02:29:14.556067       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0110 02:29:25.552301       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0110 02:29:27.553857       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0110 02:29:41.556723       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0110 02:31:35.543171       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="reactory/reactory-prometheus-7b8cfb77bd" duration="54.501µs"
I0110 02:31:49.575262       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="reactory/reactory-prometheus-7b8cfb77bd" duration="84.751µs"
I0110 02:33:46.125904       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0110 02:34:25.564036       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0110 02:34:38.566826       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0110 02:34:39.559066       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0110 02:34:50.564421       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0110 02:36:42.871663       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="reactory/reactory-prometheus-7b8cfb77bd" duration="181.707µs"
I0110 02:36:55.575479       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="reactory/reactory-prometheus-7b8cfb77bd" duration="69.375µs"
I0110 02:38:51.952163       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0110 02:39:31.553311       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0110 02:39:42.571399       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0110 02:39:45.562448       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0110 02:39:57.559594       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0110 02:41:55.426728       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="reactory/reactory-prometheus-7b8cfb77bd" duration="112.084µs"
I0110 02:42:07.562488       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="reactory/reactory-prometheus-7b8cfb77bd" duration="255.293µs"
I0110 02:43:56.941380       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0110 02:44:37.561398       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0110 02:44:49.561328       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0110 02:44:53.557638       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0110 02:45:08.557188       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0110 02:47:05.729542       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="reactory/reactory-prometheus-7b8cfb77bd" duration="156.46µs"
I0110 02:47:06.762222       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="reactory/reactory-prometheus-7b8cfb77bd" duration="55.543µs"
I0110 02:49:02.829613       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0110 02:49:49.562194       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0110 02:49:54.565536       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0110 02:50:04.566006       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0110 02:50:05.567356       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"


==> kube-proxy [126cefdaf2ed] <==
I0110 02:52:45.816679       1 server_linux.go:66] "Using iptables proxy"
E0110 02:52:45.833215       1 proxier.go:734] "Error cleaning up nftables rules" err=<
	could not run nftables command: /dev/stdin:1:1-24: Error: Could not process rule: Operation not supported
	add table ip kube-proxy
	^^^^^^^^^^^^^^^^^^^^^^^^
 >
E0110 02:52:45.836486       1 proxier.go:734] "Error cleaning up nftables rules" err=<
	could not run nftables command: /dev/stdin:1:1-25: Error: Could not process rule: Operation not supported
	add table ip6 kube-proxy
	^^^^^^^^^^^^^^^^^^^^^^^^^
 >
I0110 02:52:45.847760       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["10.211.55.4"]
E0110 02:52:45.847795       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0110 02:52:45.860086       1 server_linux.go:146] "No iptables support for family" ipFamily="IPv6"
I0110 02:52:45.860101       1 server.go:245] "kube-proxy running in single-stack mode" ipFamily="IPv4"
I0110 02:52:45.860134       1 server_linux.go:169] "Using iptables Proxier"
I0110 02:52:45.860750       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0110 02:52:45.861081       1 server.go:483] "Version info" version="v1.31.0"
I0110 02:52:45.861086       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0110 02:52:45.864010       1 config.go:197] "Starting service config controller"
I0110 02:52:45.864023       1 shared_informer.go:313] Waiting for caches to sync for service config
I0110 02:52:45.864031       1 config.go:104] "Starting endpoint slice config controller"
I0110 02:52:45.864034       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0110 02:52:45.864271       1 config.go:326] "Starting node config controller"
I0110 02:52:45.864277       1 shared_informer.go:313] Waiting for caches to sync for node config
I0110 02:52:45.964161       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0110 02:52:45.964180       1 shared_informer.go:320] Caches are synced for service config
I0110 02:52:45.964445       1 shared_informer.go:320] Caches are synced for node config


==> kube-proxy [bf31ba2912e3] <==
I0110 01:35:10.814586       1 server_linux.go:66] "Using iptables proxy"
E0110 01:35:10.839879       1 proxier.go:734] "Error cleaning up nftables rules" err=<
	could not run nftables command: /dev/stdin:1:1-24: Error: Could not process rule: Operation not supported
	add table ip kube-proxy
	^^^^^^^^^^^^^^^^^^^^^^^^
 >
E0110 01:35:10.843503       1 proxier.go:734] "Error cleaning up nftables rules" err=<
	could not run nftables command: /dev/stdin:1:1-25: Error: Could not process rule: Operation not supported
	add table ip6 kube-proxy
	^^^^^^^^^^^^^^^^^^^^^^^^^
 >
I0110 01:35:10.859088       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["10.211.55.4"]
E0110 01:35:10.859114       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0110 01:35:10.897004       1 server_linux.go:146] "No iptables support for family" ipFamily="IPv6"
I0110 01:35:10.897018       1 server.go:245] "kube-proxy running in single-stack mode" ipFamily="IPv4"
I0110 01:35:10.897033       1 server_linux.go:169] "Using iptables Proxier"
I0110 01:35:10.897612       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0110 01:35:10.898249       1 server.go:483] "Version info" version="v1.31.0"
I0110 01:35:10.898255       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0110 01:35:10.899965       1 config.go:197] "Starting service config controller"
I0110 01:35:10.899993       1 shared_informer.go:313] Waiting for caches to sync for service config
I0110 01:35:10.900095       1 config.go:104] "Starting endpoint slice config controller"
I0110 01:35:10.900098       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0110 01:35:10.900490       1 config.go:326] "Starting node config controller"
I0110 01:35:10.900493       1 shared_informer.go:313] Waiting for caches to sync for node config
I0110 01:35:11.000992       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0110 01:35:11.000992       1 shared_informer.go:320] Caches are synced for node config
I0110 01:35:11.001005       1 shared_informer.go:320] Caches are synced for service config


==> kube-scheduler [82749c02569f] <==
I0110 01:35:07.572508       1 serving.go:386] Generated self-signed cert in-memory
W0110 01:35:09.482098       1 requestheader_controller.go:196] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0110 01:35:09.482124       1 authentication.go:370] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0110 01:35:09.482129       1 authentication.go:371] Continuing without authentication configuration. This may treat all requests as anonymous.
W0110 01:35:09.482134       1 authentication.go:372] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0110 01:35:09.526518       1 server.go:167] "Starting Kubernetes Scheduler" version="v1.31.0"
I0110 01:35:09.526534       1 server.go:169] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0110 01:35:09.530049       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0110 01:35:09.530123       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0110 01:35:09.530147       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0110 01:35:09.530159       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0110 01:35:09.631152       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kube-scheduler [a2fc778192c3] <==
I0110 02:52:43.390999       1 serving.go:386] Generated self-signed cert in-memory
W0110 02:52:45.042430       1 requestheader_controller.go:196] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0110 02:52:45.042446       1 authentication.go:370] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0110 02:52:45.042452       1 authentication.go:371] Continuing without authentication configuration. This may treat all requests as anonymous.
W0110 02:52:45.042454       1 authentication.go:372] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0110 02:52:45.079702       1 server.go:167] "Starting Kubernetes Scheduler" version="v1.31.0"
I0110 02:52:45.079821       1 server.go:169] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0110 02:52:45.081265       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0110 02:52:45.082408       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0110 02:52:45.082483       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0110 02:52:45.082525       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0110 02:52:45.182976       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
W0110 04:55:32.891344       1 reflector.go:484] runtime/asm_arm64.s:1222: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0110 04:55:34.354697       1 reflector.go:561] runtime/asm_arm64.s:1222: failed to list *v1.ConfigMap: Get "https://10.211.55.4:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=279874": dial tcp 10.211.55.4:8443: connect: network is unreachable
E0110 04:55:34.355178       1 reflector.go:158] "Unhandled Error" err="runtime/asm_arm64.s:1222: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get \"https://10.211.55.4:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=279874\": dial tcp 10.211.55.4:8443: connect: network is unreachable" logger="UnhandledError"
W0110 04:55:37.130305       1 reflector.go:561] runtime/asm_arm64.s:1222: failed to list *v1.ConfigMap: Get "https://10.211.55.4:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=279874": dial tcp 10.211.55.4:8443: connect: network is unreachable
E0110 04:55:37.130437       1 reflector.go:158] "Unhandled Error" err="runtime/asm_arm64.s:1222: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get \"https://10.211.55.4:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=279874\": dial tcp 10.211.55.4:8443: connect: network is unreachable" logger="UnhandledError"
W0110 04:55:40.401937       1 reflector.go:561] runtime/asm_arm64.s:1222: failed to list *v1.ConfigMap: Get "https://10.211.55.4:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=279874": dial tcp 10.211.55.4:8443: connect: network is unreachable
E0110 04:55:40.402069       1 reflector.go:158] "Unhandled Error" err="runtime/asm_arm64.s:1222: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get \"https://10.211.55.4:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=279874\": dial tcp 10.211.55.4:8443: connect: network is unreachable" logger="UnhandledError"
W0110 04:55:52.339381       1 reflector.go:561] runtime/asm_arm64.s:1222: failed to list *v1.ConfigMap: Get "https://10.211.55.4:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=279874": dial tcp 10.211.55.4:8443: connect: network is unreachable
E0110 04:55:52.339454       1 reflector.go:158] "Unhandled Error" err="runtime/asm_arm64.s:1222: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get \"https://10.211.55.4:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=279874\": dial tcp 10.211.55.4:8443: connect: network is unreachable" logger="UnhandledError"


==> kubelet <==
Jan 11 00:17:47 minikube kubelet[1407]: E0111 00:17:47.149807    1407 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.3@sha256:a320a50cc91bd15fd2d6fa6de58bd98c1bd64b9a6f926ce23a600d87043455a3\\\"\"" pod="ingress-nginx/ingress-nginx-admission-patch-c2zbw" podUID="7a3f50e7-6f55-46ed-bc90-153e2a444f03"
Jan 11 00:17:57 minikube kubelet[1407]: E0111 00:17:57.147424    1407 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.3@sha256:a320a50cc91bd15fd2d6fa6de58bd98c1bd64b9a6f926ce23a600d87043455a3\\\"\"" pod="ingress-nginx/ingress-nginx-admission-create-6d464" podUID="48178f96-2efe-4ca0-a3e4-efd787224fc3"
Jan 11 00:17:58 minikube kubelet[1407]: E0111 00:17:58.151371    1407 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.3@sha256:a320a50cc91bd15fd2d6fa6de58bd98c1bd64b9a6f926ce23a600d87043455a3\\\"\"" pod="ingress-nginx/ingress-nginx-admission-patch-c2zbw" podUID="7a3f50e7-6f55-46ed-bc90-153e2a444f03"
Jan 11 00:18:09 minikube kubelet[1407]: E0111 00:18:09.152748    1407 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.3@sha256:a320a50cc91bd15fd2d6fa6de58bd98c1bd64b9a6f926ce23a600d87043455a3\\\"\"" pod="ingress-nginx/ingress-nginx-admission-patch-c2zbw" podUID="7a3f50e7-6f55-46ed-bc90-153e2a444f03"
Jan 11 00:18:10 minikube kubelet[1407]: E0111 00:18:10.149588    1407 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.3@sha256:a320a50cc91bd15fd2d6fa6de58bd98c1bd64b9a6f926ce23a600d87043455a3\\\"\"" pod="ingress-nginx/ingress-nginx-admission-create-6d464" podUID="48178f96-2efe-4ca0-a3e4-efd787224fc3"
Jan 11 00:18:20 minikube kubelet[1407]: E0111 00:18:20.159180    1407 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.3@sha256:a320a50cc91bd15fd2d6fa6de58bd98c1bd64b9a6f926ce23a600d87043455a3\\\"\"" pod="ingress-nginx/ingress-nginx-admission-patch-c2zbw" podUID="7a3f50e7-6f55-46ed-bc90-153e2a444f03"
Jan 11 00:18:23 minikube kubelet[1407]: E0111 00:18:23.147555    1407 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.3@sha256:a320a50cc91bd15fd2d6fa6de58bd98c1bd64b9a6f926ce23a600d87043455a3\\\"\"" pod="ingress-nginx/ingress-nginx-admission-create-6d464" podUID="48178f96-2efe-4ca0-a3e4-efd787224fc3"
Jan 11 00:18:34 minikube kubelet[1407]: E0111 00:18:34.152511    1407 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.3@sha256:a320a50cc91bd15fd2d6fa6de58bd98c1bd64b9a6f926ce23a600d87043455a3\\\"\"" pod="ingress-nginx/ingress-nginx-admission-patch-c2zbw" podUID="7a3f50e7-6f55-46ed-bc90-153e2a444f03"
Jan 11 00:18:35 minikube kubelet[1407]: E0111 00:18:35.149363    1407 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.3@sha256:a320a50cc91bd15fd2d6fa6de58bd98c1bd64b9a6f926ce23a600d87043455a3\\\"\"" pod="ingress-nginx/ingress-nginx-admission-create-6d464" podUID="48178f96-2efe-4ca0-a3e4-efd787224fc3"
Jan 11 00:18:40 minikube kubelet[1407]: E0111 00:18:40.171152    1407 iptables.go:577] "Could not set up iptables canary" err=<
Jan 11 00:18:40 minikube kubelet[1407]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
Jan 11 00:18:40 minikube kubelet[1407]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Jan 11 00:18:40 minikube kubelet[1407]:         Perhaps ip6tables or your kernel needs to be upgraded.
Jan 11 00:18:40 minikube kubelet[1407]:  > table="nat" chain="KUBE-KUBELET-CANARY"
Jan 11 00:18:47 minikube kubelet[1407]: E0111 00:18:47.148061    1407 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.3@sha256:a320a50cc91bd15fd2d6fa6de58bd98c1bd64b9a6f926ce23a600d87043455a3\\\"\"" pod="ingress-nginx/ingress-nginx-admission-create-6d464" podUID="48178f96-2efe-4ca0-a3e4-efd787224fc3"
Jan 11 00:18:48 minikube kubelet[1407]: E0111 00:18:48.146798    1407 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.3@sha256:a320a50cc91bd15fd2d6fa6de58bd98c1bd64b9a6f926ce23a600d87043455a3\\\"\"" pod="ingress-nginx/ingress-nginx-admission-patch-c2zbw" podUID="7a3f50e7-6f55-46ed-bc90-153e2a444f03"
Jan 11 00:18:53 minikube kubelet[1407]: E0111 00:18:53.146036    1407 pod_workers.go:1301] "Error syncing pod, skipping" err="unmounted volumes=[webhook-cert], unattached volumes=[], failed to process volumes=[]: context deadline exceeded" pod="ingress-nginx/ingress-nginx-controller-bc57996ff-m24pp" podUID="55606123-4bdf-4b24-a61e-6057f2518709"
Jan 11 00:18:59 minikube kubelet[1407]: E0111 00:18:59.147792    1407 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.3@sha256:a320a50cc91bd15fd2d6fa6de58bd98c1bd64b9a6f926ce23a600d87043455a3\\\"\"" pod="ingress-nginx/ingress-nginx-admission-patch-c2zbw" podUID="7a3f50e7-6f55-46ed-bc90-153e2a444f03"
Jan 11 00:19:01 minikube kubelet[1407]: E0111 00:19:01.148261    1407 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.3@sha256:a320a50cc91bd15fd2d6fa6de58bd98c1bd64b9a6f926ce23a600d87043455a3\\\"\"" pod="ingress-nginx/ingress-nginx-admission-create-6d464" podUID="48178f96-2efe-4ca0-a3e4-efd787224fc3"
Jan 11 00:19:10 minikube kubelet[1407]: E0111 00:19:10.016871    1407 secret.go:188] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Jan 11 00:19:10 minikube kubelet[1407]: E0111 00:19:10.017130    1407 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/55606123-4bdf-4b24-a61e-6057f2518709-webhook-cert podName:55606123-4bdf-4b24-a61e-6057f2518709 nodeName:}" failed. No retries permitted until 2025-01-11 00:21:12.017069516 +0000 UTC m=+19171.939946781 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/55606123-4bdf-4b24-a61e-6057f2518709-webhook-cert") pod "ingress-nginx-controller-bc57996ff-m24pp" (UID: "55606123-4bdf-4b24-a61e-6057f2518709") : secret "ingress-nginx-admission" not found
Jan 11 00:19:10 minikube kubelet[1407]: E0111 00:19:10.150109    1407 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.3@sha256:a320a50cc91bd15fd2d6fa6de58bd98c1bd64b9a6f926ce23a600d87043455a3\\\"\"" pod="ingress-nginx/ingress-nginx-admission-patch-c2zbw" podUID="7a3f50e7-6f55-46ed-bc90-153e2a444f03"
Jan 11 00:19:12 minikube kubelet[1407]: E0111 00:19:12.150101    1407 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.3@sha256:a320a50cc91bd15fd2d6fa6de58bd98c1bd64b9a6f926ce23a600d87043455a3\\\"\"" pod="ingress-nginx/ingress-nginx-admission-create-6d464" podUID="48178f96-2efe-4ca0-a3e4-efd787224fc3"
Jan 11 00:19:24 minikube kubelet[1407]: E0111 00:19:24.149041    1407 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.3@sha256:a320a50cc91bd15fd2d6fa6de58bd98c1bd64b9a6f926ce23a600d87043455a3\\\"\"" pod="ingress-nginx/ingress-nginx-admission-create-6d464" podUID="48178f96-2efe-4ca0-a3e4-efd787224fc3"
Jan 11 00:19:25 minikube kubelet[1407]: E0111 00:19:25.148369    1407 pod_workers.go:1301] "Error syncing pod, skipping" err="unmounted volumes=[reactory-env-file], unattached volumes=[], failed to process volumes=[]: context deadline exceeded" pod="reactory/reactory-express-server-5d6f4cdd55-zxczj" podUID="122a3820-011a-4f95-9c85-aafde7e4318b"
Jan 11 00:19:25 minikube kubelet[1407]: E0111 00:19:25.148746    1407 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.3@sha256:a320a50cc91bd15fd2d6fa6de58bd98c1bd64b9a6f926ce23a600d87043455a3\\\"\"" pod="ingress-nginx/ingress-nginx-admission-patch-c2zbw" podUID="7a3f50e7-6f55-46ed-bc90-153e2a444f03"
Jan 11 00:19:35 minikube kubelet[1407]: E0111 00:19:35.149423    1407 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.3@sha256:a320a50cc91bd15fd2d6fa6de58bd98c1bd64b9a6f926ce23a600d87043455a3\\\"\"" pod="ingress-nginx/ingress-nginx-admission-create-6d464" podUID="48178f96-2efe-4ca0-a3e4-efd787224fc3"
Jan 11 00:19:38 minikube kubelet[1407]: E0111 00:19:38.149569    1407 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.3@sha256:a320a50cc91bd15fd2d6fa6de58bd98c1bd64b9a6f926ce23a600d87043455a3\\\"\"" pod="ingress-nginx/ingress-nginx-admission-patch-c2zbw" podUID="7a3f50e7-6f55-46ed-bc90-153e2a444f03"
Jan 11 00:19:40 minikube kubelet[1407]: E0111 00:19:40.179724    1407 iptables.go:577] "Could not set up iptables canary" err=<
Jan 11 00:19:40 minikube kubelet[1407]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
Jan 11 00:19:40 minikube kubelet[1407]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Jan 11 00:19:40 minikube kubelet[1407]:         Perhaps ip6tables or your kernel needs to be upgraded.
Jan 11 00:19:40 minikube kubelet[1407]:  > table="nat" chain="KUBE-KUBELET-CANARY"
Jan 11 00:19:43 minikube kubelet[1407]: E0111 00:19:43.373742    1407 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/122a3820-011a-4f95-9c85-aafde7e4318b-reactory-env-file podName:122a3820-011a-4f95-9c85-aafde7e4318b nodeName:}" failed. No retries permitted until 2025-01-11 00:21:45.373672176 +0000 UTC m=+19205.296549357 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "reactory-env-file" (UniqueName: "kubernetes.io/host-path/122a3820-011a-4f95-9c85-aafde7e4318b-reactory-env-file") pod "reactory-express-server-5d6f4cdd55-zxczj" (UID: "122a3820-011a-4f95-9c85-aafde7e4318b") : hostPath type check failed: /Users/wweber/Source/reactory/reactory-express-server/config/reactory/.env.container is not a file
Jan 11 00:19:46 minikube kubelet[1407]: E0111 00:19:46.154865    1407 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.3@sha256:a320a50cc91bd15fd2d6fa6de58bd98c1bd64b9a6f926ce23a600d87043455a3\\\"\"" pod="ingress-nginx/ingress-nginx-admission-create-6d464" podUID="48178f96-2efe-4ca0-a3e4-efd787224fc3"
Jan 11 00:19:50 minikube kubelet[1407]: E0111 00:19:50.149127    1407 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.3@sha256:a320a50cc91bd15fd2d6fa6de58bd98c1bd64b9a6f926ce23a600d87043455a3\\\"\"" pod="ingress-nginx/ingress-nginx-admission-patch-c2zbw" podUID="7a3f50e7-6f55-46ed-bc90-153e2a444f03"
Jan 11 00:19:58 minikube kubelet[1407]: E0111 00:19:58.152128    1407 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.3@sha256:a320a50cc91bd15fd2d6fa6de58bd98c1bd64b9a6f926ce23a600d87043455a3\\\"\"" pod="ingress-nginx/ingress-nginx-admission-create-6d464" podUID="48178f96-2efe-4ca0-a3e4-efd787224fc3"
Jan 11 00:20:02 minikube kubelet[1407]: E0111 00:20:02.145140    1407 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.3@sha256:a320a50cc91bd15fd2d6fa6de58bd98c1bd64b9a6f926ce23a600d87043455a3\\\"\"" pod="ingress-nginx/ingress-nginx-admission-patch-c2zbw" podUID="7a3f50e7-6f55-46ed-bc90-153e2a444f03"
Jan 11 00:20:09 minikube kubelet[1407]: E0111 00:20:09.147029    1407 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.3@sha256:a320a50cc91bd15fd2d6fa6de58bd98c1bd64b9a6f926ce23a600d87043455a3\\\"\"" pod="ingress-nginx/ingress-nginx-admission-create-6d464" podUID="48178f96-2efe-4ca0-a3e4-efd787224fc3"
Jan 11 00:20:14 minikube kubelet[1407]: E0111 00:20:14.153608    1407 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.3@sha256:a320a50cc91bd15fd2d6fa6de58bd98c1bd64b9a6f926ce23a600d87043455a3\\\"\"" pod="ingress-nginx/ingress-nginx-admission-patch-c2zbw" podUID="7a3f50e7-6f55-46ed-bc90-153e2a444f03"
Jan 11 00:20:21 minikube kubelet[1407]: E0111 00:20:21.148880    1407 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.3@sha256:a320a50cc91bd15fd2d6fa6de58bd98c1bd64b9a6f926ce23a600d87043455a3\\\"\"" pod="ingress-nginx/ingress-nginx-admission-create-6d464" podUID="48178f96-2efe-4ca0-a3e4-efd787224fc3"
Jan 11 00:20:27 minikube kubelet[1407]: E0111 00:20:27.147685    1407 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.3@sha256:a320a50cc91bd15fd2d6fa6de58bd98c1bd64b9a6f926ce23a600d87043455a3\\\"\"" pod="ingress-nginx/ingress-nginx-admission-patch-c2zbw" podUID="7a3f50e7-6f55-46ed-bc90-153e2a444f03"
Jan 11 00:20:34 minikube kubelet[1407]: E0111 00:20:34.146639    1407 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.3@sha256:a320a50cc91bd15fd2d6fa6de58bd98c1bd64b9a6f926ce23a600d87043455a3\\\"\"" pod="ingress-nginx/ingress-nginx-admission-create-6d464" podUID="48178f96-2efe-4ca0-a3e4-efd787224fc3"
Jan 11 00:20:40 minikube kubelet[1407]: E0111 00:20:40.166618    1407 iptables.go:577] "Could not set up iptables canary" err=<
Jan 11 00:20:40 minikube kubelet[1407]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
Jan 11 00:20:40 minikube kubelet[1407]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Jan 11 00:20:40 minikube kubelet[1407]:         Perhaps ip6tables or your kernel needs to be upgraded.
Jan 11 00:20:40 minikube kubelet[1407]:  > table="nat" chain="KUBE-KUBELET-CANARY"
Jan 11 00:20:40 minikube kubelet[1407]: E0111 00:20:40.361270    1407 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" image="registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.3@sha256:a320a50cc91bd15fd2d6fa6de58bd98c1bd64b9a6f926ce23a600d87043455a3"
Jan 11 00:20:40 minikube kubelet[1407]: E0111 00:20:40.361345    1407 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" image="registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.3@sha256:a320a50cc91bd15fd2d6fa6de58bd98c1bd64b9a6f926ce23a600d87043455a3"
Jan 11 00:20:40 minikube kubelet[1407]: E0111 00:20:40.361526    1407 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:patch,Image:registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.3@sha256:a320a50cc91bd15fd2d6fa6de58bd98c1bd64b9a6f926ce23a600d87043455a3,Command:[],Args:[patch --webhook-name=ingress-nginx-admission --namespace=$(POD_NAMESPACE) --patch-mutating=false --secret-name=ingress-nginx-admission --patch-failure-policy=Fail],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:POD_NAMESPACE,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gklh2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:*false,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,AppArmorProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod ingress-nginx-admission-patch-c2zbw_ingress-nginx(7a3f50e7-6f55-46ed-bc90-153e2a444f03): ErrImagePull: Error response from daemon: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" logger="UnhandledError"
Jan 11 00:20:40 minikube kubelet[1407]: E0111 00:20:40.363362    1407 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ErrImagePull: \"Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-patch-c2zbw" podUID="7a3f50e7-6f55-46ed-bc90-153e2a444f03"
Jan 11 00:20:47 minikube kubelet[1407]: E0111 00:20:47.146958    1407 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.3@sha256:a320a50cc91bd15fd2d6fa6de58bd98c1bd64b9a6f926ce23a600d87043455a3\\\"\"" pod="ingress-nginx/ingress-nginx-admission-create-6d464" podUID="48178f96-2efe-4ca0-a3e4-efd787224fc3"
Jan 11 00:20:53 minikube kubelet[1407]: E0111 00:20:53.146931    1407 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.3@sha256:a320a50cc91bd15fd2d6fa6de58bd98c1bd64b9a6f926ce23a600d87043455a3\\\"\"" pod="ingress-nginx/ingress-nginx-admission-patch-c2zbw" podUID="7a3f50e7-6f55-46ed-bc90-153e2a444f03"
Jan 11 00:21:01 minikube kubelet[1407]: E0111 00:21:01.354991    1407 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" image="registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.3@sha256:a320a50cc91bd15fd2d6fa6de58bd98c1bd64b9a6f926ce23a600d87043455a3"
Jan 11 00:21:01 minikube kubelet[1407]: E0111 00:21:01.355141    1407 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" image="registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.3@sha256:a320a50cc91bd15fd2d6fa6de58bd98c1bd64b9a6f926ce23a600d87043455a3"
Jan 11 00:21:01 minikube kubelet[1407]: E0111 00:21:01.355416    1407 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:create,Image:registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.3@sha256:a320a50cc91bd15fd2d6fa6de58bd98c1bd64b9a6f926ce23a600d87043455a3,Command:[],Args:[create --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc --namespace=$(POD_NAMESPACE) --secret-name=ingress-nginx-admission],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:POD_NAMESPACE,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9fk88,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:*false,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,AppArmorProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod ingress-nginx-admission-create-6d464_ingress-nginx(48178f96-2efe-4ca0-a3e4-efd787224fc3): ErrImagePull: Error response from daemon: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" logger="UnhandledError"
Jan 11 00:21:01 minikube kubelet[1407]: E0111 00:21:01.357204    1407 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ErrImagePull: \"Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-create-6d464" podUID="48178f96-2efe-4ca0-a3e4-efd787224fc3"
Jan 11 00:21:06 minikube kubelet[1407]: E0111 00:21:06.157630    1407 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.3@sha256:a320a50cc91bd15fd2d6fa6de58bd98c1bd64b9a6f926ce23a600d87043455a3\\\"\"" pod="ingress-nginx/ingress-nginx-admission-patch-c2zbw" podUID="7a3f50e7-6f55-46ed-bc90-153e2a444f03"
Jan 11 00:21:08 minikube kubelet[1407]: E0111 00:21:08.144560    1407 pod_workers.go:1301] "Error syncing pod, skipping" err="unmounted volumes=[webhook-cert], unattached volumes=[], failed to process volumes=[]: context deadline exceeded" pod="ingress-nginx/ingress-nginx-controller-bc57996ff-m24pp" podUID="55606123-4bdf-4b24-a61e-6057f2518709"


==> storage-provisioner [3f3c5214faa6] <==
I0110 22:47:34.391930       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0110 22:47:34.394852       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0110 22:47:34.394865       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0110 22:48:20.400439       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0110 22:48:20.401936       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_9b1e39e7-9d33-437a-98e2-58e202afc8b6!
I0110 22:48:20.402389       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"4ef60683-ecfe-40ee-b061-f542b31c9014", APIVersion:"v1", ResourceVersion:"290506", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_9b1e39e7-9d33-437a-98e2-58e202afc8b6 became leader
I0110 22:48:20.505989       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_9b1e39e7-9d33-437a-98e2-58e202afc8b6!


==> storage-provisioner [b78d6302ec40] <==
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1.3()
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x54
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0x400055c0c0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:155 +0x64
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0x400055c0c0, 0x1267368, 0x40004a0c90, 0x1, 0x4000090780)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:156 +0x74
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0x400055c0c0, 0x3b9aca00, 0x0, 0x1, 0x4000090780)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:133 +0x88
k8s.io/apimachinery/pkg/util/wait.Until(0x400055c0c0, 0x3b9aca00, 0x4000090780)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x48
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x308

goroutine 82 [sync.Cond.Wait]:
sync.runtime_notifyListWait(0x40003e6b10, 0x3)
	/usr/local/go/src/runtime/sema.go:513 +0xf8
sync.(*Cond).Wait(0x40003e6b00)
	/usr/local/go/src/sync/cond.go:56 +0xb8
k8s.io/client-go/util/workqueue.(*Type).Get(0x40003fa9c0, 0x0, 0x0, 0x1c200)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/util/workqueue/queue.go:145 +0x84
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).processNextClaimWorkItem(0x4000428500, 0x1298cd0, 0x40003e6dc0, 0x0)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:935 +0x34
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).runClaimWorker(...)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:924
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1.2()
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:880 +0x54
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0x400055c0e0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:155 +0x64
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0x400055c0e0, 0x1267368, 0x40004a0cc0, 0x1, 0x4000090780)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:156 +0x74
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0x400055c0e0, 0x3b9aca00, 0x0, 0x1, 0x4000090780)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:133 +0x88
k8s.io/apimachinery/pkg/util/wait.Until(0x400055c0e0, 0x3b9aca00, 0x4000090780)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x48
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:880 +0x3c8

goroutine 83 [sync.Cond.Wait]:
sync.runtime_notifyListWait(0x40003e6b50, 0x0)
	/usr/local/go/src/runtime/sema.go:513 +0xf8
sync.(*Cond).Wait(0x40003e6b40)
	/usr/local/go/src/sync/cond.go:56 +0xb8
k8s.io/client-go/util/workqueue.(*Type).Get(0x40003fab40, 0x0, 0x0, 0x1c200)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/util/workqueue/queue.go:145 +0x84
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).processNextVolumeWorkItem(0x4000428500, 0x1298cd0, 0x40003e6dc0, 0x400007e001)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:990 +0x34
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).runVolumeWorker(...)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:929
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1.3()
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x54
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0x400055c100)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:155 +0x64
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0x400055c100, 0x1267368, 0x40004a06c0, 0x1, 0x4000090780)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:156 +0x74
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0x400055c100, 0x3b9aca00, 0x0, 0x1, 0x4000090780)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:133 +0x88
k8s.io/apimachinery/pkg/util/wait.Until(0x400055c100, 0x3b9aca00, 0x4000090780)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x48
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x308

